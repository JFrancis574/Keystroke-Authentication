\documentclass[10pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{wrapfig}
\usepackage{graphicx}
\graphicspath{{./Images/}}
\usepackage{listings}
\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
\pagenumbering{roman}

\title{Using Keystroke Dynamics to Authenticate a User Based on their Typing}
\author{Jack Francis}
\date{\today}


\begin{document}

\begin{titlepage}
    \begin{center}
        \vspace*{1cm}
            
        \Huge
        \textbf{Using Keystroke Dynamics to Authenticate a User Based on their Typing}
            
        \vspace{0.5cm}
        \LARGE
        CO2301 Computer Science Project Dissertation
            
        \vspace{1.5cm}
            
        \textbf{Jack Francis}
            
        \vfill
            
        \vspace{0.8cm}
            
        \Large
        School of Computing and Mathematical Sciences\\
        University Of Leicester\\
        \today
            
    \end{center}
\end{titlepage}

\newpage
PLAG DEC


\chapter*{Abstract}

Many security systems in use today are one-time systems such as passwords and biometrics are only useful at login. If a user has authenticated and then an imposter takes control, these systems are useless. This paper presents an approach for a continuous system which uses keystroke dynamics along with 2D correlation co-efficient and Dynamic Time Warping to ensure that the user is genuine and if an imposter user is detected will force the user to re-authenticate. The system works around two main features, Words and Intervals. Data is collected in intervals, formed into word and then validation is performed on these words using KD signals, Dynamic Time Warping, Euclidean Distance and Correlation Co-efficient.The system also contains a training phase which is designed to ensure that the learning process is useful along with a play/pause functionality which ensures that the users privacy is respected. The produced system which is based along these lines is fast, secure and accurate and fulfils the aims and objective set out at the start of the project. The system is also designed to take as little space on the users storage as possible through the introduction of a compression and decompression functions. The system also contains a simple update function which allows it to learn users.  The system boasts an accuracy of \(87.5\%\) which is very good and should ensure that the user feels protected and secure at all times.

\tableofcontents

\chapter{Introduction}
\pagenumbering{arabic}
\setcounter{page}{1}

In today's world, passwords aren't enough to guarantee security. New methods such as fingerprints and facial recognition have been tried in order to improve security. However, whilst these methods are good for login, they don't prevent malicious users taking control after a user has logged in. In this use, case they essentially become useless. This is where keystroke dynamics are useful, it can catch these imposter users who have bypassed the other authentication methods. By analysing the keystrokes and building up a profile for the user, a keystroke dynamics-based system can easily ensure the system is being used by a legitimate user. Most systems that attempt to do this are theoretical in nature and most if not all use a machine learning approach. This is where my system differs from others because it uses an approach build on KD signals and a Dynamic Time Warping algorithm. This has many advantages such as speed and performance as many other machine learning based approaches need significant time and resources in order to learn how a user types.

The system has been created in python 3.9 and has an emphasis on security, accuracy and performance. The key aims and objectives that I originally set out to achieve and based my system on are shown below:

\begin{enumerate}
	\item To produce a lightweight key logger that can accurately log all inputs accurately and securely
	\item To create a graphical system that allows the user to register or login
	\item At an interval set by the user, the system will create a profile for that user based on their typing since the last interval and check this against the profile created in the registration system
	\item If the user doesn't match, asks the user to re-authenticate
\end{enumerate}

Overall, the aim of the project is to create a system that can authenticate users based on their keystroke dynamics that is fast, accurate and secure.

\chapter{Survey of Literature}

In order to create this project, it was necessary to do some research into the techniques and approaches. 

In order to do some research onto what a system based on keystroke authentication would involve, I read \emph{Time-frequency analysis of keystroke dynamics for user authentication}\cite{ToosiRamin2021Taok} which presented a highly theoretical one time system that used a maths based approach using a dynamic time warping algorithm along with a Wigner distribution in order to perform similarity calculations. The presented system only worked with small word samples. Unfortunately, the system wasn't a continuous system, but the paper still provided value in how to implement a validation measure. The validation function in my produced system is a highly modified version of the system set out in this paper. I decided to rely on this paper because it was published recently, well-cited, well researched and presented a solid approach I couldn't find any problems with.

Another paper I read in the opening stages of this project is \emph{Keystroke dynamics-based authentication service for cloud computing: Keystroke Authentication Service.}\cite{cloudComp} This paper showcased a continuous machine learning approach as opposed to the one time maths based approach showcased previously. Whilst I did not use the approach set out in this paper, the different viewpoint showcased was useful when making the decision in what approach to take. The reason I didn't end up choosing this approach because whilst the paper is relatively recent, it is not well-cited and the paper lacks detail in what is actually happening. Furthermore, I choose to go with a maths based approach because I felt this was more challenging. I decided I couldn't rely on this paper because of these reasons and as such it wasn't that useful to me.

Once I had made my decision in what approach to take, I read papers about algorithms used in certain steps of my validation function. The paper \emph{FastDTW: Toward Accurate Dynamic Time Warping in Linear Time and Space}\cite{Salvador2004FastDTWTA} presented the Dynamic Time Warping algorithm. The paper has been well-cited, presented the algorithm in great detail, was easy to understand and really helped me get an understanding of what the algorithm does. The paper is also peer reviewed and the authors well reputed. Therefore this paper was useful to me as it allowed me to get an understanding of one of the key algorithms in my project.

Whilst doing research on Dynamic Time Warping, I also read \emph{Fuzzy clustering of time series data using dynamic time warping distance}\cite{IzakianHesam2015Fcot} which used Dynamic Time Warping in order to improve their clustering algorithm. Whilst the  use case is not exactly the same as mine, the paper was still useful in order to see how they had prepared their data and what the results after going through the algorithm was. The paper is not well-cited but still proved to be useful due to it's subject matter and the clear approach stated within. This paper and the previous one allowed me to get a clear and deep understanding of how to implement and what the results are of the algorithm and as such are useful. 

I also did some research into the Wigners Distribution which I was planning on implementing inside the validation procedure. The paper \emph{Wigner Distribution}\cite{inbook} originally published in 2009 provided an exploration of this distribution on the context of quantum physics. Whilst the subject matter was different and it was difficult to understand to of the quantum maths elements which limited my understanding, I would still class this paper as useful. It allowed me to make a decision if this was strictly necessary to implement in my program and if the proposed algorithm provided enough accuracy to overcome it's performance hit. \emph{The Wigner Distribution: A Time-Frequency analysis tool}\cite{4766782} further increased my understanding of the distribution and proved useful in explaining details which were not explained by the previous papers.

Research into open-source examples was also undertaken with a look at a keystroke analysis system developed by Nikolai Janakiev\cite{janakiev_2018}. He created a theoretical one time system which uses machine learning in order to create models and work out how a user typed. This was useful as it was interesting to see how other people have approached the project. The approach detailed in there is based on the paper \emph{Comparing anomaly-detection algorithms for keystroke dynamics}\cite{5270346} which was of huge help when formulating my approach even though I didn't follow this approach.

The final paper I read was \emph{Keystroke Dynamics-Based Authentication Using Unique Keypad}\cite{s21062242} which showcased an approach using a unique keypad that the user uses to login. Whilst the keypad element isn't useful in my case, the approach they used to record and process the data from the keypad was useful and influenced my decision making when it came to the system. However, I didn't end up relying on this paper due to the highly theoretical nature of the system and it was not capable of being easily translated into my system.

\chapter{Design and Implementation}

I decided to create my system in python due to is flexibility, the large amount of documentation available, and my familiarity with it. Furthermore, python contains multiple packages such as NumPy, which makes doing advanced maths simple, easy, and provides excellent performance. Due to how my program operates, many data points are generated for each word. As such, having fast packages and function is essential to keeping the program as lightweight as possible and avoiding a potential performance impact on the user.

\begin{wrapfigure}{l}{0.5\textwidth}
	\begin{center}
		\includegraphics[width=0.48\textwidth]{OOP}
	\end{center}
	\caption{Basic Class Structure}
	\label{fig:ClassStruct}
\end{wrapfigure}

It also makes interacting with the various elements of my system simple due to a large number of packages available. I used an object-orientated approach in my system. Figure \ref{fig:ClassStruct} is a visual representation of the class structure used in my program. The main file is where the program starts and runs from. When the raw data is collected in the main file, it is passed into the Calculation class where the preprocessing, word forming, and validation procedures happen. Training inherits from the Calculation class and shares all methods and attributes with a saving to file function simply added on. The User profile is self-explanatory and just provides a simple way to store the user's details. The Word class contains details of each word produced by the program. It's a simple storage mechanism with many functions, such as generating the KD signal and compression attached among others.

\begin{figure}
	\includegraphics[scale=0.7]{OldPlan}
	\caption{Original System Plan}
	\label{fig:OldPlan}
\end{figure}

\begin{figure}
	\includegraphics[scale=0.55]{SystemPlan}
	\caption{Actual System Plan}
	\label{fig:NewPlan}
\end{figure}

My system is a relatively complex system that differs slightly from the one set out in my interim report. My plan from my interim report is shown in figure \ref{fig:OldPlan}. It's significantly more straightforward than my actual system plan shown in figure \ref{fig:NewPlan}. One main difference is the lack of a database to hold the user login information. Upon creating such a database, I felt this was unnecessary and, as such, omitted this from my final system in favour of using a Windows-based authentication system. This was done as too many authentication systems can confuse the user, and there is no better security than the windows one. A new feature in my final system plan is the play/pause UI which allows the user to pause the system should they do something involving sensitive information they do not want to be recorded. Another new step is the update step; if the user initially fails validation but then re-authenticates correctly using their windows username and password, the relevant word objects are updated. I went for a maths-based approach rather than using popular approaches such as machine learning because machine learning approaches typically have a lengthy learning phase that consumes a lot of the user's time and a lot of the user's system resources. Furthermore, a maths-based approach allows the system to be much more lightweight, and it's simpler to develop and for the user to understand what is going on. 

The entire system works at intervals. An interval is defined as a period in which the data is collected. For example, if the interval were 60 seconds, then data would be collected for 60 seconds, processed, and then the next interval would start. If the interval was shorter, then while the program would be theoretically more secure, it would suffer from a lack of data collected, which would affect the system's overall accuracy. Furthermore, having a shorter interval would lead to the program doing many more calculations, which would affect the user's system performance. Choosing an interval of 60 seconds allows me to balance accuracy and performance. While I tried other interval times such as 5, 30, and 45 seconds, these all suffered from a lack of data collected or started to affect performance severely. In particular, the 5-second interval never collected enough data to be able to make a call and sometimes took longer than five seconds to perform the calculations, which started to cause a problem due to the gaps in coverage. 

Each step is explained in more detail below. The user, upon first login, starts in the registration procedure. It is here that the system learns how the user types. It creates the profile data storage and then stores the data learned inside. The system then records the user's keystrokes for the interval. Once this has been completed, the data goes through the preprocessing steps and is then formed into words. For each interval, only a select few words are chosen for validation. Once the words are chosen, they are put through validation. This involves loading the learned data from the profile data storage and comparing the two using validation procedures. If they pass validation, the system moves on to the next interval and repeats the steps. If the words fail validation, the user is asked to re-authenticate using windows authentication UI. If they pass this, the failed words are updated in the validation procedure, and the system continues. If the user fails, a new user is created using the registration system. 

While all this is happening, the play/pause UI is running in a separate thread. Should the user ask the program to pause using this UI, they are asked to re-authenticate, and if this succeeds, the whole system is stopped until the user asks the program to continue again. 

The validation implementation is a rough version of one shown by Ramin Toosi and Mohammad Ali Akhaee in their excellent paper 'Time–frequency analysis of keystroke dynamics for user authentication'. \cite{ToosiRamin2021Taok} The paper is theoretical and describes an approach for performing validation on one word and then comparing them. It is, in essence, a one-time system, while mine is a continuous system that aims to keep the user safe. I have implemented their validation approach in my project while adding data gathering, word-forming, word selection, word storage, and update functionality. In figure \ref{fig:ValPlans}, you can see the two maths backends compared to one another. The main difference is that Wigners Distribution is omitted in the actual system. This is because this distribution is slow in my use case, and the improvements in accuracy are not worth the performance loss. The other difference between the two is the introduction of the Euclidean distance. This was introduced as a secondary validation measure to allow greater validation. This and the Correlation coefficient feed into the final similarity score.

The validation implementation is a rough version of one shown by Ramin Toosi and Mohammad Ali Akhaee in their excellent paper 'Time–frequency analysis of keystroke dynamics for user authentication'. \cite{ToosiRamin2021Taok} The paper is theoretical in nature and describes an approach for performing validation on one word and then comparing them. It is in essence a one-time system whilst mine is a continuous system that aims to keep the user safe. In my project, I've modified and implemented their validation approach whilst adding data gathering, word forming, word selection, word storage and update functionality. In figure \ref{fig:ValPlans} you can see the two maths backends compared to one another. The main difference between the two is that Wigners Distribution is omitted in the actual system. This is because this distribution is slow in my usecase and the improvements in accuracy are not worth the performance loss. The other difference between the two is the introduction of the Euclidean distance. This was introduced as a secondary validation measure in order to allow the greater validation. It's this and Correlation co efficient that feed into the final similarity score.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{MathsBackend}
		\caption{Interim report maths backend}
		\label{fig:IRMB}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{NewMaths}
		\caption{Actual System maths backend}
		\label{fig:ACMB}
	\end{subfigure}
	\caption{New and Old Validation Plans}
	\label{fig:ValPlans}
\end{figure}

\section{Data gathering and Forming}
My program relies on capturing the user's keystrokes, processing them and comparing them using a similarity measure. In order to do this, I decided to use the Keyboard Library\cite{boppreh_2016} as it is a lightweight, secure and modern library that makes capturing keystrokes easy. In my project, I use of the hook function of the library, which is used to "hook" onto a user's keyboard and record all the user's actions and create keyboard events for each action. The record function which makes use of this hook function is shown in figure \ref{fig:Record}. The code snippet is very simple; first the program will 'hook' onto the keyboard using the Keyboard Library mentioned above, record all keystrokes until the interval has passed and then stop recording. The start time of the interval and an array of Keyboard Events are then returned to the main body of the program. The start time of the interval is recorded and returned as it used further on in order to be able to place keyboard events on a time line in the context of the interval.

\begin{figure}[h!]
	\begin{lstlisting}
		def record(interval):
    		recorded = []
    		startTime = time.time()
    		keyBoardHook = keyboard.hook(recorded.append)
    		time.sleep(interval)
    		keyboard.unhook(keyBoardHook)
    		return recorded, startTime
	\end{lstlisting}
	\caption{Record Function}
	\label{fig:Record}
\end{figure}

A keyboard event is generated every time the user does something on the keyboard, whether pressing or releasing a key. Further information such as the type of the action (whether it was a press or a release), which key is this action happening on and a highly accurate time stamp of when the event occurred is also recorded. Figure \ref{fig:Hook}, shows an example of a keyboard event produced by the function when the user presses down the 'h' key.

\begin{figure}[h]
\centering
\includegraphics[scale=0.97]{KeyboardEvent}
\caption{The keyboard representation of a user pressing "h"}
\label{fig:Hook}
\end{figure}	

The first element in the array is the action, this can be either 'up' or 'down'. The next field is the scan-code, which I don't use but is useful for identifying keys. The next field is the name of the key which in this case is 'h'. After this is the time since the epoch in seconds, which is useful as this precise time-stamp is used to do the rest of the calculations. The other three fields are device, modifiers and whether the user uses a keypad. None of these I use in my program and are discarded almost immediately.

A small amount of pre-processing is then done on this data before it is paired up. The first step is to remove the scan code, keypad, modifier and device from each keyboard event and in order to make them lighter and more usable. The next step is to take the start time that is returned by the record function and subtract this from the time stamp in each keyboard event to get the time that the action occurred in the interval. Figure \ref{fig:preproc} shows what the data in \ref{fig:Hook} looks like after going through this.

\begin{wrapfigure}{l}{0.5\textwidth}
	\begin{center}
		\includegraphics[width=0.48\textwidth]{KeyboardEventPreProc}
	\end{center}
	\caption{Keyboard representation of a user pressing 'h' after pre-processing}
	\label{fig:preproc}
\end{wrapfigure}

The data collected at this point is stored as a 2D array with each sub array corresponding to an action. An example sub array is shown in \ref{fig:preproc}.

In this form, the data cannot be used for anything, as it currently takes the form of several individual actions that have no relation to one another. Therefore, the next step is to form pairs from the data. A pair is formed of one 'down' and one 'up' action where the key field matches and the 'down' is before the 'up'. This is done is that it allows the program to work with half as much data, which reduces the number of unnecessary data points and allows the program to be able to form words using these pairs.

\begin{wrapfigure}{r}{0.4\textwidth}
	\begin{center}
		\includegraphics[width=0.38\textwidth]{SimplePairing}
	\end{center}
	\caption{Example of simple pairing}
	\label{fig:SimpPair}
\end{wrapfigure}

There are two main rules to follow when pairing the data. In many cases, the user will press and release a key in quick succession without pressing any other keys. Due to the chronological nature of the data, pairing these types of presses is easy. All that is needs to be done is to iterate through the pairs and when we come to a 'down' action then simply select the next value in the array if it is an 'up' action and the key matches. Figure \ref{fig:SimpPair} shows an example of this type of pairing. However, this type of nice easy matching is not always the case.

In some cases, a user may press multiple keys down at once. For example, this might occur when the user is capitalising words using 'shift' or when the user is typing fast, so they may be already pressing down the next key before releasing the previous. An example of what the data will look like when this is the case is shown in Figure \ref{fig:WrongPair}. Applying the previous method in which we pair up keys with matching key types and opposite actions which are next to each other would result in the output shown in the pairs array. As you can see, this is not correct and would only lead to one pair where there should be two.

\begin{wrapfigure}{r}{0.4\textwidth}
	\begin{center}
		\includegraphics[width=0.38\textwidth]{PairingWrong}
	\end{center}
	\caption{Example of simple pairing}
	\label{fig:WrongPair}
\end{wrapfigure}

In order to fix this, it is necessary to include another case in the code. Suppose the current action does not have a matching key and opposite action next in the array. In that case, the program will iterate through the rest of the array starting from the current point, searching for the next entry with a matching key and an opposite action after the current. If it finds one it will pair them up. The key thing we assume for this to work is that every action has an opposite action. In nearly all cases, we assume this to be true as it is improbable that a user will hold down a key for the entire interval. If the method cannot find a resultant action event, then it is still added to the pairs list with an end time of the length of the interval.

The resulting pairing algorithm has a time complexity of \(O(n^2)\). In a program that is all about speed and minimal impact to the user, it is essential that the program has the lowest time complexity as possible. Due to the complicated nature of how users type, I believe this is the best time complexity for a problem of this nature.

\subsection{Forming Words}

After forming the pairs, the next step is to form words from these pairs. The words that the program forms are essential as it is this that the program uses to compare users. In English words, take many forms; as such, it is needed to account for numerous possibilities in the word forming function.

A \emph{word} is defined in my program as a sequence of pairs bounded by punctuation, white space, or the use of modifiers such as 'shift'. In the later stages of this report, I refer to these as break pairs. The one notable exception to this rule is when an apostrophe or a hyphen pair is detected. If this occurs, then the program will check the previous pair and the pair afterward, and if both are letters and not numeric or punctuation, the pair, including the punctuation is added to the current word. 

The data at this stage takes the form of a 2D array. The program will iterate through the 2D array given and check the key that the pair matches. If it is a letter or a number, it is added to another array used to store the current word being formed. If a break pair is found, it is not added to the current word, and the the current word is used to form a word object which is then saved to an output array. If the break pair is a white space pair, then the pair is skipped. However, if the break pair is a modifier such as 'shift' or 'ctrl' then the semantics dictionary's relevant entry is updated for that user. This dictionary is used in the validation section of the project and is another indicator of how a user types. Backspace handling is done separately. If the user has pressed backspace, then the last letter added to the word is removed from it. The program can handle multiple backspaces even if they delete the entirety of the current word. If this occurs, the last word object is popped off the array to be the current word, and the last letter of this new current word is removed.

When the program gets to the last pair in the input array, if the pair is not a break pair, the pair is added to the current word, and the current word forms a word object which is then saved to the output array.

The program will then return the output array which at this time is formed of word objects and the semantics dictionary. The output array is then saved to the wordsOut attribute of the Calculation clas,s while the semantics is saved to the semantics attribute in the class.

\subsection{Data Selection}

If the program was to go through and check every single word for similarities, the cost in terms of time would be high and would make the program unfit for use, especially if the user typed quickly during the interval. For example, if the program checked every word and the user typed 56 words in a 60-second interval, the time taken would be over 2 minutes, as shown in figure \ref{fig:WordsvsTime} which, while highly accurate and secure, would render the program unusable as the time taken to process and perform all the similarity calculations would be in excess of the interval and as such would lead to a lower degree of accuracy and security. Furthermore, this would severely impact the performance of the user's computer and, as such go against one of the main aims of the project.

It is necessary to use a sampling method to choose words from the list of words generated by the word forming function. While this is less accurate than checking every word, the performance gain over checking every word is vast with a significant time saving per interval. Choosing how many words were selected was the next problem I endeavoured to fix.

I conducted a number of tests measuring how long the entire validation procedure took. Initially, I started with two words chosen per interval, with one chosen every at the start and one at the end. I then increased the number of words chosen by two each time, with the interval remaining the same. The interval remained the same with a word selected evenly throughout the interval. Figure \ref{fig:WordsvsTime} shows the results of such a test. The time taken to perform the calculation increases linearly as the number of words chosen increases. If the word chosen is particularly long, then the time taken increases further. The test data was the same for all tests with the user typing a paragraph containing 57 words of differing lengths.

\begin{figure}
	\centering
	\includegraphics[scale=0.70]{WordsChosenVsTime}
	\caption{Comparing words chosen to time taken in seconds}
	\label{fig:WordsvsTime}
\end{figure}

After performing the test, I settled on the program choosing four words per interval. Choosing four words took under 10 seconds and allowed the program to get coverage every quarter of the interval, which is acceptable. In addition choosing the words is simple; it will take one word from each quarter of the interval, allowing the program to get good coverage.

\section{KD Signal}
Once the raw keystroke data has been formed into words and the words chosen, the next step is to transform the data from a word object made up of keystroke pairs into numerical data that later algorithms can use. The best way to do this is to transform the data into a measure of how many keys are being held down at a particular point in time. The resulting output is known as a key down signal (KDS). \cite{ToosiRamin2021Taok}

The start and end times of the word being transformed are used to convert a word into a key down signal. Assume that \(w\) is the array of times that key actions occur in a particular word. \(w_1\) is the time of the first action, and \(w_n\) is the time of the last action. This part will loop through all timestamps until it ends with the final time, denoted by \(w_n\). The accuracy of this step is paramount as it is the level of detail that is the base accuracy for the rest of the steps. Higher accuracy means that the program will check more data points within this range at the cost of reduced performance. This is set to 4 decimal points which seems to provide a good balance between accuracy and performance.
\begin{equation}
\textit{KDS}(w) = \sum^{w_n}_{i=w_1}K(w_i)
\end{equation}
\(K\) is the next step of the algorithm and is heavily based on the KDS algorithm shown in \cite{ToosiRamin2021Taok}. \(n\) is the array of key presses that is used in the previous step. This step of the algorithm iterates through all the key presses and uses a modified Heaviside step function denoted by \(h\), which is run twice per pair with the time input from the previous denoted as \(t\) and the 'down' action denoted as \(n_i^1\) and the 'up' denoted as \(n_i^2\). The value returned by the Heaviside step function with the 'up' action is subtracted from the value returned by the 'down' function.
\begin{equation}
\textit{K}(t) = \sum^{n_k}_{x=1}h(t,n_i^1)-h(t,n_i^2)
\end{equation}
The reason for this subtraction is that the purpose of this measure is to return the number of keys pressed down at the time input. Once a key has been released, it must be removed from the measure. For example, if a pair exists with down action time being at 1 second and up action time being 1.1 seconds. At time, 1.5 the equation will equal \(1-1=0\). However, if the time put in is 1.05, then the equation will be \(1-0=1\) which indicates that one key was being held down at this particular time.

For every time input, each pair is checked with the sum of all the results stored in a dictionary along with the time input as the key. This dictionary that forms the KD signal and is used in further steps.

\subsection{Heaviside Step Function}
This is the bottom layer of the KD signal algorithm. It is a modified version of the Heaviside Step function.
\begin{equation}
	h(x_1, x_2) = \begin{cases}
	1 & \text{ if } x_1 > x_2 \\
	0.5 & \text{ if } x_1 == x_2 \\
	0 & \text{ if } x_1 < x_2
\end{cases}
\end{equation}
The modification done is straightforward; the only change is the addition of a third case which tests if the two times are equal. Due to the nature of the use case for my project, there is a relatively high chance that the two times are equal to one another. In this case, this means that the user at this time is currently in the process of performing that action, whether that be pressing or releasing the key. The theoretical basis for this function is taken from "Time–frequency analysis of keystroke dynamics for user authentication" by Ramin Toosi and Mohammad Ali Akhaee\cite{ToosiRamin2021Taok}

\subsection{Output}
The resulting signal can be shown easily in graph format. For example, figure \ref{fig:KDS1} and figure \ref{fig:KDS2} show the same user typing the same word twice in two intervals. Figure \ref{fig:KDSBoth} shows both sets of data overlaid on the same graph. As you can see, even with the same user typing it, both sets look very different, and normalisation becomes essential in order to be able to compare the data accurately. 

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{KDS1}
		\caption{First user typing hello}
		\label{fig:KDS1}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{KDS2}
		\caption{Second user typing}
		\label{fig:KDS2}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{KDSBoth}
		\caption{Both signals overlayed on each other}
		\label{fig:KDSBoth}
	\end{subfigure}
	\caption{KD Signals generated by a user typing hello}
	\label{fig:KDS}
\end{figure}

\section{Dynamic Time Warping}

The data in my project requires normalising to be useful. The algorithm that I chose to use is called the Dynamic Time Warping algorithm\cite{Salvador2004FastDTWTA}. Two sets of data, one loaded in from the word file and the other generated from that interval, is input into this algorithm in order to normalise it. As both sets of data are input, the Dynamic Time Warping algorithm attempts to change both sets of data in order to make them as similar as possible. The output is two sets of normalised data which can then be compared against one another to provide a similarity score. 

I chose to use the fastdtw library \cite{tandi_2015}. As the name suggests, this library provides a fast implementation of the dynamic time warping algorithm with the algorithm taking only \(O(n)\) time complexity\cite{tandi_2015}.

The output of the two datasets shown in figure \ref{fig:KDS1} and figure \ref{fig:KDS2}, both put through dynamic time warping is shown in figure \ref{fig:KDSDTW}.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{KDS1DTW}
		\caption{First after DTW}
		\label{fig:KDS1DTW}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{KDS2DTW}
		\caption{Second after DTW}
		\label{fig:KDS2DTW}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{KDSBothDTW}
		\caption{Both signals overlayed on each other}
		\label{fig:KDSBothDTW}
	\end{subfigure}
	\caption{KD signals after going through DTW}
	\label{fig:KDSDTW}
\end{figure}

Figures \ref{fig:KDS1DTW} and \ref{fig:KDS2DTW} are the results of the data in figures \ref{fig:KDS1} and \ref{fig:KDS2} after going through dynamic time warping. Despite going through the process, you can see that they have not changed much and look the same. This is because both signals were typed by the same user and are very similar. However, the first signal has an extra datapoint on the end, which causes it to go on longer than the other signal. However, this is not a huge issue, as the validation systems explained below returned the values of \(1.0\) and \(0.99999\) which showcases that both signals are similar and were typed by the same user.

\subsection{Path}
The implementation i chose to use returns two values, the distance and the path. The distance is used in the validation step to calculate the euclidean distance. The path is more important and used to "warp" the data. Theoretically, it is the least costly path through a cost matrix. It essentially is an array of pairs of indexes in the two datasets that map up the two input signals.

\section{Validation Measures}

Finally, the data is in a state where we can perform validation. In order to improve the accuracy of the program, I use two different validation methods along with the semantics collected in the word forming stage. 

The two methods I use along with the semantics are Euclidean distance which is done in the dynamic time warping stage and 2D correlation coefficient. Euclidean distance has secondary importance compared to the 2D correlation co-efficient. This is because the Euclidean distance is simply a measure of the distance between two points and is not a good indicator for data such as this which has a variable number of points and as such this value is highly variable. Therefore, I use this as a rough figure and use the correlation co-efficient as the primary method. 

The values produced by these validation methods are then put together to form a value between 0 and 100. This is then compared against the confidence level. The confidence level is a integer which determines how similar the data loaded in from the file, and the data collected in the interval have to be. For example, if the validation methods in the interval (Similarity Score) return a value of 0.75 and the confidence level is 0.8, the user is rejected and asked to re-authenticate. When developing this project, I settled on a confidence level of 0.8 because this maximised the accuracy of decision making and minimised errors. Only the Euclidean distance and the 2D correlation go into calculate the similarity score. The semantics are used to either raise  or lower the confidence level depending on the value returned by the semantics validation.

\begin{wrapfigure}{r}{0.4\textwidth}
	\begin{center}
		\includegraphics[width=0.38\textwidth]{SemanticsEx}
	\end{center}
	\caption{Example of a users semantics file}
	\label{fig:sem}
\end{wrapfigure}

The semantics validation is elementary. In the words step of the validation procedure, the semantics of the data input is collected and stored in the class. Each user will also have a semantics file stored in their data directory. An example of a user's semantics file is shown in figure \ref{fig:sem}. The data stored in the file and the data collected for that interval are then compared against one another. If, for example, the user in the past has used the shift key and had also done so in this interval, then the confidence level is lowered by 0.02. However, if the user in the past has always used the caps lock key and never shift, then they do not match and the confidence level is raised by 0.02. The reason of such a small change in the confidence level is that user's habits regarding this are not typically set in stone, and as such, any large change will likely lead to reduced accuracy.

The validation methods are explained further in sections 3.4.1 and 3.4.2 respectively. Here I will outline the entire validation process. The flow chart in figure \ref{fig:valProc} provides a visual representation of the first part of the validation procedure.

\begin{wrapfigure}{l}{0.4\textwidth}
	\begin{center}
		\includegraphics[width=0.38\textwidth]{DistancesDict}
	\end{center}
	\caption{Example of a distances dictionary after validation calculations done}
	\label{fig:distances}
\end{wrapfigure}

\begin{figure}
	\centering
	\includegraphics[scale=0.3]{ValidationProc}
	\caption{Validation Procedure}
	\label{fig:valProc}
\end{figure}

Figure \ref{fig:valProc} provides a high level look at the entire validation procedure. As shown in figure \ref{fig:valProc}, the words chosen in the first step are iterated through. If a word already has a word file stored in the user's directory, that is loaded in and decompressed. Otherwise, the procedure returns two None values. The two data sets, one loaded in and the other generated from the current interval, are put through Dynamic Time Warping, where the Euclidean distance is calculated and stored. The path generated by the DTW is then used to 'warp' both datasets. Finally, The 2D correlation coefficient is used in order to calculate one element of the similarity score. The Euclidean distance and the correlation coefficient are then stored in a dictionary, with the key being the index in the chosen array. Figure \ref{fig:distances} showcases an example distances dictionary. In the example, the first two elements were typed by a genuine user, with the third being typed by an imposter and the last word has not been seen before, so the validation procedures just returned None.

After the distances dictionary has been formed with all values inside. The next step is to categorise the values. First, we have to define the confidence level for each validation procedure. The current values are 0.8 for the correlation coefficient and 1000 for the Euclidean distance. As shown in the flowchart (\ref{fig:valProc}), these are adjusted based on the results of the semantics validation. Then, the distances dictionary is iterated through, and each value checked. Figure \ref{fig:DistanceTransform} below shows what happens in each case where \(e\) is the Euclidean Distance, \(c\) is the value returned by the correlation co-efficient, \(i\) is the confidence level for the correlation co-efficient, and \(b\) is the confidence level for the Euclidean Distance.

\begin{equation}
	\begin{cases}
		\textit{None} & e == \textit{None or } c == \textit{None} \\
		\textit{True} & c >= i \textit{ and } e <= b \\
		\textit{True} & c >= i \textit{ and } e >= b \\
		\textit{False} & \textit{Otherwise}
	\end{cases}
	\label{fig:DistanceTransform}
\end{equation}

The value returned by this function is then added to an array used in the next step. Figure \ref{fig:wordCheck} shows the resulting array formed by putting the data in \ref{fig:distances} through this part of the validation procedure. This example presumes that the confidence levels for Euclidean Distance and correlation co-efficient are 1000 and 0.80, respectively.

\begin{figure}[h]
	\centering
	\includegraphics{WordCheck}
	\caption{The result of the data in figure \ref{fig:distances} after going through further validation}
	\label{fig:wordCheck}
\end{figure}

Transforming the data this way allows the order of the data to be preserved. Furthermore, it allows the next stage of the validation to be simplified heavily as rather than working with variable data, we now only need to only consider whether the value is one of True, False or None which vastly simplifies the complicated system used to make a decision which is explained in the next step.

The "wordCheck" array shown in \ref{fig:wordCheck} is then used in the next step to finally perform an action based on the data. Despite what happens next, only three outcome occur. Figure \ref{fig:If1} showcases what happens if only one word is chosen and the respective outcomes. If the outcome is true, then the person passes validation, and the program moves on to the next interval. However, further processing  must be done if the word does not pass the validation checks or has never been seen before.

If the user fails validation, their pc is locked, and they are forced to re-authenticate. In order to perform this action, I load in the default python library sub process \cite{subprocess} which I then use to call the windows lock dll file. This enables me to lock the user out of the computer easily and efficiently without much-wasted code or overhead in terms of performance. If the user has changed when the user does re-authenticate, then 'New' is returned. This tells my main body of the program that it is a new user and as such a new keyboard is created for that user. If the opposite happens and the user is re-authenticated successfully, the word files and semantics are updated and the program returns true.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{IfLen1}
	\caption{Flowchart which illustrates decision tree if only one word is chosen}
	\label{fig:If1}
\end{figure}

The validation function can take in as a parameter a 'mode'. This determines what happens should validation fail. The default mode is 'r' which stands for real. This allows the program to do the entire process including locking the users computer if validation fails. 'rnl' stands for real no lock, which is mainly used for testing and demonstration purposes because it stops the users pc locking and just returns false. A 't' flag also exists; this indicates that the program is in test mode, and when in test mode the program will save every word.

Typically the user will type multiple words in an interval. As the result outputted above from the wordCheck can take any one of three values for each word, there are plenty of variations. These are all explained below.

\textbf{Possibility 1: All words pass Validation, and wordCheck consists of just True values}. True and an empty array are returned. The runner which runs the validation process then continues on to the next interval.

\textbf{Possibility 2: Some words pass validation, and some have never been seen before}.The program creates word files for those not been seen for using the update function which is detailed in \ref{sssec:update}. True and an empty array is then returned, and the program moves on to the next interval.

\textbf{Possibility 3: All words have never been seen before or fail validation}. When a combination of never seen before and failing validation is detected, the program has no confidence that the user is genuine; therefore, it starts the re-authentication procedure explained above.

\textbf{Possibility 4: The chosen only consists of words which the program has never been seen before}. Much like possibility 3, the program has no way of knowing whether the user is genuine or not. Therefore it takes the safest course of action and forces the user to re-authenticate. It then does the same as possibility 3 does.

\textbf{Possibility 5: All words fail validation}. If all words fail validation, the program is confident that an imposter is attempting to use the computer. The function locks the pc and demands re-authentication from the user. Like in possibility 3 and 4, it will update the words that fail validation if the user passes re-authentication.

\subsection{Euclidean Distance}

The first validation method used is the Euclidean Distance. It is a system of measure between two data points. In my system it is used on the warped data in order to generate the distance between them. The overall score is calculated by adding all the distances between the data points output by the KD signal function and the data points input from the word file. This is results in an outstanding benchmark figure to determine how close the two signals are. 

\begin{equation}
\textit{euc}(a,b) = \sqrt{\Sigma^n_{i=1}(a_i-b_i)^2}
\label{fig:EUC}
\end{equation}

Figure \ref{fig:EUC} shows the mathematical formula for Euclidean distance, where \(a\) and \(b\) are the two KD signals, and \(n\) is the number of data points. This then returns a value, as we can see in the first element of the array in \ref{fig:VCompare}. The value returned is self-explanatory with a smaller value indicating less difference between signals than a large one. In my program, rather than calculate this value myself, I used scipys implementation of this function\cite{scipycommunity_2022}. Attempting to do this myself in pure python resulted in slow processing times. The function used by scipy is much faster due to it being written in C++, which is far better at doing these functions with large datasets produced by the KD signal function.

\subsection{Correlation Coefficient}

\begin{figure}
	\centering
	\includegraphics[scale=0.5]{EucVs2D}
	\caption{Euclidean Distance compared to 2D correlation co-efficient}
	\label{fig:VCompare}
\end{figure}

2D correlation co-efficient is used because it is relatively lightweight, quick, easy to implement, and provides easy to interpret results. As opposed to the value produced by the euclidean distance, the output returned by the 2D correlation co-efficient can only take a range of values between -1 and 1. If the function returns 1, then the two signals input are exactly the same, 0 if they are radically different, and -1 if they are the same again. An example of the output returned can be seen in the second element of the dictionary in figure \ref{fig:distances}. In shortened form, the 2D correlation figure essentially returns a percentage of how close the two input signals are.

This measure is used after the dynamic time warping and take the two normalised signals as inputs. This is weighted higher in my system due to the stronger accuracy and ease of use. One example of why the this is more accurate than euclidean distance is shown in figure \ref{fig:VCompare}. 2D correlation co-efficient returns around 0.95 for both, whist euclidean distance returns 180 and 300 which are wildly differing figures thus making classification difficult.
\begin{equation}
	f(a,b) = \frac{\Sigma^n_{i=1}(a_i-\overline{a})(b_i - \overline{b})}{\sqrt{\Sigma^n_{i=1} (a_i-\overline{a})^2}\sqrt{\Sigma^n_{i=1}(b_i-\overline{b})^2}}
	\label{fig:corrco}
\end{equation}

Figure \ref{fig:corrco} shows the equation that my implementation is based upon, where a and b are the two warped arrays passed in, \(n\) is the length of either array, \(a_i\) and \(b_i\) are the values at positions \(i\) in each array respectively. My code implementation is shown in figure \ref{fig:CodeCorr}.

\begin{figure}[h!]
	\begin{lstlisting}
		# Correlation Coefficient
        cov = 0
        XSum = 0
        YSum = 0
        Xmean = sum(ff_warped)/len(ff_warped)
        Ymean = sum(ii_warped)/len(ii_warped)
                    
       	for i in range(len(ff_warped)):
        	cov += (ff_warped[i] - Xmean)*(ii_warped[i] - Ymean)
            XSum += math.pow(ff_warped[i]-Xmean, 2)
            YSum += math.pow(ii_warped[i]-Ymean, 2)
                            
        correlationCoefficient = cov/((math.sqrt(XSum)*(math.sqrt(YSum))))
       \end{lstlisting}
       \caption{Code Implementation of correlation co-efficient}
       \label{fig:CodeCorr}
\end{figure}

Again, the code is self-explanatory, with the Xmean and Ymean variables simply calculating the mean for the two input arrays. The output of this function is then stored along with the euclidean distance in a dictionary to be used for further processing.

\section{Training}
When a new user is created, it is essential that training happens. Otherwise, the program has nothing to go on and, as such, locks the computer at every interval. This is incredibly annoying for the user, but the learning process would also be slow. Therefore it is necessary to add a training phase. There are two potential approaches to a training phase. The first is to have a dedicated text that the user types out and the second is that for the first \(x\) intervals, all word data is saved and then used as a bank. I explored both of these attempts in my project. 

I first explored the dedicated training phase with a dedicated sample text that the user types out. The difficulty with this was not making the UI or ensuring that the data was captured; it was choosing the text itself. For this to work correctly, a text was needed that contained all of the most common words that a user uses, along with a wide range of punctuation. This in itself is difficult as each user is different. My program currently uses the text shown below as the dedicated text.

"Since they are still preserved in the rocks for us to see, they must have been formed quite recently, that is geologically speaking?
What can explain these striations and their common orientation?			
Did you ever hear about the Great Ice Age or the Pleistocene Epoch?			
Less than one million years ago	in fact	some 12000 years ago an ice sheet many thousands of feet thick rode over Burke Mountain in a southeastward direction.
The many boulders frozen to the underside of the ice sheet tended to scratch the rocks over which they rode.			
The scratches or striations seen in the park rocks were caused by these attached boulders.				
The ice sheet also plucked and rounded Burke Mountain into the shape it possesses today."

This piece of text was chosen as it was long enough to get a wide variety of punctuation in but so long that the user gets bored. A wide variety of words is also used in an attempt to get the best coverage available.

The next approach I tested was having the first five intervals for a new user simply recording and saving all used words. This gives better coverage than the other approach because it allows the program to learn the user’s habits and common words. This approach also deals well with different users and allows greater insight into the user. While this method is good at learning users, it potentially opens up a security issue. If all words are saved and stored, nothing stops an imposter user from altering this training data. This approach has another disadvantage; it does not learn how the user uses punctuation which the other approach does.

While exploring both of these, I concluded that a mixture of both training methods would be the best approach. This would ensure that the program gets the best of both worlds. Therefore, I decided to use both systems, with the first five intervals after the user finishes training being the training intervals. Five was chosen as the number of training intervals because it minimises the security risk caused by an imposter user while still proving to be an effective data gathering tool.

\begin{figure}
	\begin{lstlisting}
		def runner(id, prof, stop):
    		count = 0
    		while True:
        		data, start = record(interval)
        		if stop():
            		break
        		if trainingItersYN == False or count > trainingIters:
            		inter = i.Calculation(data, start, prof, 1)
            		if stop():
                		break
            		decision, index = inter.validation(mode='r')
           			if decision == False:
                		break
        		else:
           			if count <= trainingIters and len(data) != 0:
                		inter = t.Training(data, start, prof, 1,0)
                		count += 1
                		if stop():
                   			break
      \end{lstlisting}
      \caption{The main runner function with training intervals in}
      \label{fig:runner}
\end{figure}

The code in \ref{fig:runner} highlights the main "runner" code. This takes the data in from the record function; if training iterations are turned on and the number of training iterations has been exceeded, the validation function is run with that data, and a decision is returned. If that happens, the pc goes through the authentication specified by the validation procedure. However, if the opposite is true and it is a training interval, then the data is saved as specified by the training class, which saves the KD signal generated for that word.

\section{Update}
\label{sssec:update}

The way users type changes over time. This is caused by several factors, such as age or change in desk setup. For example, how a user types at home will differ from how users type at a coffee shop. Therefore, it is essential that my program can learn and adapt. This is where the update function comes in. This step is done after the validation step and is called in possibility where a user has failed validation but re-authenticated successfully. Essentially this is called where the program has got it wrong. 
The function is straightforward. It will iterate through all words that have failed validation and update the word files with the data collected from the current interval.

\section{Storage}
\label{sssec:Storage}
For this project to work correctly, it is essential that the program has a way of storing how users type certain words. Therefore, it is necessary to build up a storage of the user’s words in the past. In my project, this is done by storing the KDS generated in the KD step and storing it in a JSON file. The reason for storing like this instead of storing it inside a database is that the data is highly variable, and as such, it is not possible to store the data inside a database. For example, one signal might have 90,000 data points while another might only have 5000. It is not practical to store this highly variable data inside a database. Each file is named after the word contained and represented by the KD signal inside. For example, if the word was "hello", the file would be named hello.json. Storing it this way ensured easy access to the data and simple identification of which data belonged to which individual. Storing just the KD signal instead of storing the pairs or the raw keystroke data collected in the first place has many advantages.The first major advantage of storing just the KDS is that a malicious user cannot easily read how a user types by looking at the files. Secondly, this type of data is easy to read by the program. Thirdly it stops the program from performing the KDS element of the validation procedure every time it loads in a file resulting in less performance impact. Simply loading in and comparing is far easier than processing all of the data again and comparing it. Figure \ref{fig:uncomp} shows a small excerpt of one of these word files.

The main problem with storing the data this way is that file sizes can become a problem. Generating a KD signal generates a lot of data points, and when these are all stored, the size of each word file starts to become an issue. Users also tend to use a lot of words, and once the user has been using the program long enough, the size of the user’s directory can become huge. One way to fix this would be to reduce the number of data points generated for each word. Unfortunately, this is not feasible as accuracy is negatively impacted. Therefore the solution is to compress the data points when they are saved to the file and then decompress them when loaded back in.

\begin{wrapfigure}{r}{0.4\textwidth}
	\begin{center}
		\includegraphics[width=0.38\textwidth]{UncompressedData}
	\end{center}
	\caption{A sample of an uncompressed word storage file}
	\label{fig:uncomp}
\end{wrapfigure}

Figure \ref{fig:uncomp} shows a sample of the representation of the data stored without compression, whilst figure \ref{fig:compression} shows the same data after compression. Due to the way the KD signal is generated, there are a large amount of duplicate values. My compression algorithm works by storing the range in values at which the KD signal is the same. In figure \ref{fig:compression} you can quite clearly see that between the timestamps of 1.5649 and 1.6368 the value did not change from 1. Previously all of the timestamps between these two values would have been stored, leading to a bloated file size that contained a lot of unnecessary information.

This type of compression makes a huge impact on file sizes. For example, the file storing the data in figure \ref{fig:uncomp} without compression is approximately 91 kilobytes, whilst the compressed version shown in \ref{fig:compression} is approximately 735 bytes. This is a considerable saving in terms of space, with the compressed version being 99\% smaller than the uncompressed version. Whilst 91 kilobytes may not seem like a lot, the average office worker types at approximately 30 words per minute\cite{naskar_2020} for approximately 8 hours a day, leading to around 14400 words typed per day. If even a small percentage, such as 10\% of these words are unique, then the resulting uncompressed word data would take up 131040 kilobytes or 131.4 megabytes worth of space. However, if the data is compressed then the same amount of words would only take up 1.05 megabytes. In this case, this results in savings of 130 megabytes which is a huge amount.

\begin{figure}[h!]
	\begin{lstlisting}
[
    {"key": [1.5647, 1.5647], "value": 0.0}, 
    {"key": [1.5648, 1.5648], "value": 0.5}, 
    {"key": [1.5649, 1.6368], "value": 1.0}, 
    {"key": [1.6369, 1.6369], "value": 0.5},
    {"key": [1.637, 1.664], "value": 0.0}, 
    {"key": [1.6641, 1.6641], "value": 0.5}, 
    {"key": [1.6642, 1.772], "value": 1.0}, 
    {"key": [1.7721, 1.7721], "value": 0.5}, 
    {"key": [1.7722, 1.7955], "value": 0.0}, 
    {"key": [1.7956, 1.7956], "value": 0.5}, 
    {"key": [1.7957, 1.865], "value": 1.0},
    {"key": [1.8651, 1.8651], "value": 0.5}, 
    {"key": [1.8652, 1.9731], "value": 0.0}, 
    {"key": [1.9732, 1.9732], "value": 0.5}, 
    {"key": [1.9733, 2.0206], "value": 1.0}, 
    {"key": [2.0207, 2.0207], "value": 0.5}, 
    {"key": [2.0208, 2.1287], "value": 0.0}, 
    {"key": [2.1288, 2.1288], "value": 0.5}, 
    {"key": [2.1289, 2.2002], "value": 1.0}
]
	\end{lstlisting}
	\caption{KD representation of a user typing hello after compression}
	\label{fig:compression}
\end{figure}

Performance is essential to the program's success, but reducing this slightly in order to save storage space on the user's computer is acceptable. The next step is where to store these word files.

Each user has a "User Profile". This essentially stores information about the current user such as the profile name, their current keyboard, whether they are a new user, and how many keyboards they have. When a user profile is created as a result of either a new profile being created in the training procedure or "New" being returned from the validation procedure, a new folder is created in the Data folder inside the programs home directory, which is named after the users Windows user name. The python library has a built in function called "getpass" which simply interfaces with windows in order to get the current users information and provide a secure interface for password input\cite{getuser}. In order to get the current users information, I use the getuser function from this library. Each user's keyboard is stored inside the users folder which is stored inside the data folder. The user's word information is stored inside the relevant keyboard along with their semantics storage file. A typical folder structure is shown in figure \ref{fig:foldStruct}.

It's done this way because users type differently on different keyboards and it made sense to have a way to store all the different profiles. The semantics file is also stored inside the keyboard folder. This is because users may use caps lock on one keyboard and shift on another. It ensures that the program can learn the user and their quirks. The user profile class stores the list of keyboards along with the path to the current keyboard.

\begin{wrapfigure}{l}{0.3\textwidth}
	\begin{center}
		\includegraphics[width=0.28\textwidth]{FolderStruct}
	\end{center}
	\caption{Example of folder structure after light usage by a user with one keyboard}
	\label{fig:foldStruct}
\end{wrapfigure}

\section{Pausing}

One main problem with my system was the lack of user privacy. For example, a user wouldn't want the program to pick up and store details about their bank password. Therefore, it becomes necessary to introduce a pause functionality. This is very simple and consists of a simple user interface screen which consists of one button which is shaped like a pause button. When the button is pressed the pause symbol changes to a play and the system stops recording the users keystrokes until the button is pressed again. The UI that powers this functionality runs on the main thread whilst the calculation happens on another thread. When the button is pressed to pause the calculation, the stop thread flag is switched to True which stops the thread and causes it to return. As such, when the calculation is switched on again a new thread is created and the calculation continues.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{PauseEx}
		\caption{Pause UI}
		\label{fig:pauseEx}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{PlayEx}
		\caption{Play UI}
		\label{fig:playEx}
	\end{subfigure}
	\caption{Example of play/pause UI in both states}
	\label{fig:PPUI}
\end{figure}

However, this system is open to abuse because an imposter could simply switch the system off, use the system and then switch is back on. In order to combat this, when the user decides to pause the program, they are prompted to re-authenticate using the windows login system. This essentially combats this problem and ensures that data collection remains. Figure \ref{fig:PPUI} showcase both states of this simple section of the UI. 

Figure \ref{fig:stoppingThread} showcases the function that is run everytime the button is pressed. It essentially checks what image is being displayed. If the pause image is being displayed, then the program calls the windows UI in order to authenticate the user. If they pass this, then the image is changed, the stop threads flag is switched to true and all threads stop. If the play image is being displayed, then the image is switched and the calculation sub-process is started back up again.

\begin{figure}
	\begin{lstlisting}
	def stop():
    global stop_threads
    if button.cget('image') == 'pyimage2':
        cmd='rundll32.exe user32.dll, LockWorkStation'
        subprocess.call(cmd)
        button.configure(image=imgPlay)
        button.image = imgPlay
        stop_threads = True
        for worker in workers:
            worker.join()
    else:
        button.configure(image=imgPause)
        button.image = imgPause
        stop_threads = False
        tmp = threading.Thread(target=runner, args=(0, prof, lambda: stop_threads))
        tmp.start()
        \end{lstlisting}
        \caption{Code for controlling calculation threads}
        \label{fig:stoppingThread}
\end{figure}

\chapter{Results and Discussion}

In order to be able to test the accuracy of my system it is necessary to first define some measures. False Positive (FP) is used to define the case when the system lets in an imposter user. False Negative (FN) is the opposite of this where the system rejects a user it shouldn't have. True Positive (TP) is where the system makes the correct decision and allows in a user correctly. True Negative (TN) is where the system makes the correct decision and rejects a user where it should have. Hold time (HT) is the time in which a key is held down or the time between the when a key is pressed and when a key is released. The float time (FT) is the time between actions. It can be defined as the time between the user releasing a key and pressing down the next key.

In order to test the system, it was necessary to create some data. In order to do this, I made a function which would generate test data with timings I specified. This enables me to generate lots of test data simply and easily. Furthermore, it allows reproducibility in the data collected. If the data was simply a user such as myself typing, it is almost impossible to get the same timings in terms of typing more than once. This function can be seen in Appendix C. That function can only deal with data that is linear (where a 'up' action is always followed by a 'down' action).


\section{Test 1: Validation Measures}

The first tests I conducted on the system consisted of checking if results returned by the validation procedures are what is expected. In this test, the confidence level is irrelevant as we are not measuring this. The test data consists a singular word "geographically". The first test in this set of tests consisted of me generating 10 sets of test data using the same word each time. Each set of test data increased the hold time by 0.1 whilst keeping the float time the same.. The program was in 'rnl' which ensured that it did not lock the pc or update the data. This was done in order to ensure that all tests were tested against the same dataset. The reference dataset which was loaded in each time consisted of the 5th set of test data. This test should return an inverted bell-shaped curve due to the reference data point being in the middle. 

\begin{figure}
	\centering
	\includegraphics[scale=0.6]{EUCGraph}
	\caption{Euclidean Score}
	\label{fig:EucGraph}
\end{figure}

Figure \ref{fig:EucGraph} shows the results of the testing. As shown, the test data results in a inverted bell curve which is expected. The smallest euclidean distances being in the middle as these are the most similar to the reference data that all datapoints are checked against. This proves that the euclidean distance is working as expected. Interestingly, the euclidean distance score is not a perfect inverted curve due much higher results on the left side of the graph rather than the right. I'm not too sure why this happens but I believe it may be because of the way in which Euclidean distance is calculated. It's because of results like this that the euclidean score is used as a estimation and a secondary measure to the 2D correlation co-efficient.

\begin{figure}
	\centering
	\includegraphics[scale=0.6]{CorrGraph}
	\caption{2D Correlation Co-efficient Score}
	\label{fig:2DGraph}
\end{figure}

Figure \ref{fig:2DGraph} graphs the hold time against the 2D correlation score. Similar to the graph shown in figure \ref{fig:EucGraph} it takes the shape of a bell shaped curve. Unlike the euclidean graph this is a far more uniform graph which highlights once again that the 2D correlation co-efficient is a better measure than euclidean distance. The table below shows the raw data of such a test.

\begin{center}
	\begin{tabular}{|c|c|c|c|}
	\hline
	HT & Euc & Corr & Dec \\ [0.5ex]
	\hline
	\hline
	0.1 & 13997 & 0.56 & False \\
	0.2 & 6001 & 0.72 & False \\
	0.3 & 6001 & 0.73 & False \\
	0.4 & 1.5 & 0.99 & True \\
	0.5 & 1.5 & 0.99 & True \\
	0.6 & 1.5 & 0.99 & True \\
	0.7 & 1.5 & 0.99 & True \\
	0.8 & 6000 & 0.74 & False \\
	0.9 & 4000 & 0.83 & False \\
	1.0 & 10000 & 0.58 & False \\
	\hline
	\end{tabular}
\end{center}

The raw data points to another interesting anomaly in the data, \(0.9\) if following the trend should have a euclidean distance greater than in \(0.8\) and a 2D correlation co-efficient less than in \(0.8\). However, this is not the case. This small anomaly doesn't affect the system in a huge way. Calculating the accuracy of this data we can see that 9/10 data sets follow the trend leading to an accuracy of 90%.

\section{Test 2: System Test}

The next logical thing to test in my program is the general accuracy of the decisions it makes. The best way to do this was to simulate the programs use by recording a number of intervals where certain actions happen and seeing the FP, FN, TP, TN rate. All tests were done with the same four words which were all compared against the benchmark figure installed at the start. The words were "Hello google words geographically". These were chosen as they are of different lengths and as such allow me to also test the semantics validation procedure due to the presence of capital letters. The following tests were done, the program should categorise genuine words as true and imposter as false.

\begin{enumerate}
	\item All words are genuine 
	\item Word 3 is an imposter, all others are genuine
	\item All words are imposters
	\item Words 1 and 3 are genuine, Words 2 and 4 are imposters
\end{enumerate}

Figure \ref{fig:ConfMat} shows the confusion matrix produced after running the tests. 1 indicates true and 0 false. In order to calculate this, I input the expected result as defined above, and the actual result returned by my system for each word. The results look good with 14 out of the 16 words returning the expected result. However, whilst we had no false negatives we had two false positives. This indicates an error rate of \(12.5\%\) which is acceptable for a program such as this. Thus the true accuracy of the system is \(87.5\%\) which I feel is a very good for a system of this type.
\begin{figure}
	\centering
	\includegraphics[scale=0.48]{ConfMatrixTest2}
	\caption{Confusion Matrix of Test Results}
	\label{fig:ConfMat}
\end{figure}

In the future, in order to improve the true accuracy, I could implement the Wigner distribution which has been proven to increase the accuracy along with increasing the number of data points produced per word. I would also expand the number of tests done, as a sample size of 16 words is relatively small and more words would allow me to evaluate the system better. Despite this, this test is still good and it shows the perceived accuracy of the system.

This test also allows me to measure the speed of the program. Figure \ref{fig:Time} shows the times taken to perform the validation procedure. The average time taken to perform the interval validation procedure is around 21 second, however this figure is inflated by one particularly long validation score which is around four times larger than the rest. Three of the four scores are between 7 and 17 which is a good time for the procedure and as such ensures that the program is fast enough.

\begin{figure}
	\centering
	\includegraphics{Time}
	\caption{The time taken to perform the calculations in the test}
	\label{fig:Time}
\end{figure}

\chapter{Critical Appraisal}




\section{Summary and Critical Analysis}

 Whilst the produced system differs from the word explained in the outline as shown by figures \ref{fig:OldPlan} and \ref{fig:NewPlan}, the functionality remains the same. I would go as far as to say that the produced system is an upgrade on the proposed system. The produced system is accurate, lightweight, secure and as such fulfils all of the original aims. Whilst some functionality around login has not been implemented due to not being needed, the expanded functionality added far outweighs this. The expanded functionality being the compression and decompression function which vastly reduce the amount of storage space being used in the user's computer, the update functionality which allows the system to learn the user's typing throughout the life cycle of the program and the multiple stages of the training functionality which allow the system to get a comprehensive and deep knowledge of how the user types. By changing my original plans and moving away from a database approach due to the high variability in the amount of data produced for each word by the system, I've reduced the performance cost because inserting and removing data from a database is a costly operation in terms of performance.

The word forming and choosing is also another way in which the functionality of the proposed system has been expanded in the produced system. In my proposed system, I planned on checking the entire intervals keystrokes and not forming words. By doing first forming words and then checking a small sample of these, the resulting system is faster and more accurate. Instead of forming one profile for an entire user, which is broad and not particularly accurate as a result, the system produces a mini profile for each word. The more data points per user which are generated by this approach allows the system to be more accurate.

The overall system is accurate and secure, as shown by figure \ref{fig:2DGraph}. The system is the only data that is stored is a highly compressed version of the KDS signal. No data is transmitted off the user's system as well, ensuring that user's who are concerned about the data being shared with a third party such as advertisers can use the system without worry. The addition of the play/pause functionality is also good for users who may be worried about the system recording sensitive information. Therefore, it is clear to see that the produced system is highly accurate and secure. When combined with the weightlessness of the program, its clear to see that the system is fit for use.

However, the produced system does suffer from some shortcomings. As mentioned previously, whilst an imposter user accessing the data stored inside the word files is not a concern due to the nature of the data within, the program suffers from a security standpoint due to the word files being named after the word within. However, because of the play/pause functionality, the user should pause the program every time they need to enter sensitive information which is a small relief from this issue. In the future, I would seek to encrypt the file names in order to mitigate this issue. The validation measure is only as good as the number of data points collected. Therefore, for words such as "hi" which are going to generate relatively few data points, the system struggles with providing an accurate validation score leading to a few cases of false negatives or positives. This is a hard issue to fix however, the obvious solution is simply to omit word with a few data points to be selected for validation. This throes up more problems however, what if small words are the only options in that interval? Another option would be to simply go into greater detail and generate more data points for those words. Unfortunately due to the nature of creating a UI in python, the provided UI for the system is rather ugly and not pleasant to look at. If I was to do this project again this is the main thing I would do differently.

Overall, I would say that producing the system went well with no major problems. The actual element I thought would be hardest and take the most effort in the actual validation system itself was easier than expected. In contrast, planning out the data flow from the raw keystroke data to the validation result took more planning and steps than expected. The UI was more of an issue that it needed to be. This is because  I need to play around with different threads as I needed my program to be able to do two things at once. 

\subsection{Aims and Objective}

In order to judge the success of the project, it is necessary compare the current system against the original aims and objectives of the project which are listed in the introduction.

I feel that my project has quite comfortably succeeded in point 1. The key logger produced records all keystrokes along with the action associated with them. Furthermore, the impact on performance is minimal with the keylogger in particular not being noticeable to the user thus satsifying the accurate and lightweight requirements. The validation procedure is unfortunately heavier and may slow down the user system a small amount. If I was to do the project again, I would reduce the performance impact on the user of the validation procedure by either implementing a faster procedure to generate the data points or by improving the storage solution. In order to implement a faster procedure, I would rewrite the project in a faster but more basic language such as C++. This would dramatically improve the speed of the validation procedure but would drastically increase the amount of code written due to the lack of suitable packages. Whilst other storage systems such as databases were explored, I would aim to go for a storage solution that could store highly variable data such as the one produced by this program, securely, speedily and with minimal impact on the user's performance and disk space. One example of a solution is a big data storage system which uses ai in order to store the data. This would also speed up the program because then the compression and decompression functions would no longer be needed which both take up \(O(n)\) time each. Furthermore, this would allow me to implement clustering which would further speed up the program by allowing similar record to be stored next to each other. Overall, the produced system comfortably fulfils this aim.

My project partly succeeded in point 2. I did create a registration system but not in the sense that I meant here. In my original aims and objectives, the registration system would allow the user to sign up using username and password. In my produced system, I quickly dismissed the idea of a login and registration system as explained above. This was because I felt that the need there was no need for a system such as this, because I could just use the windows registration, login and authentication systems. These provide a more secure, fast and overall better experience for the user than any I could create myself. However, I did still create a registration system, this registration system simply was simply used to learn how the user types. This was a massive success and is a relatively complicated system that is explained better in the training section of Design and Implementation. So whilst I didn't create a login system, or the exact registration system I set out in my original aims and objective, I still feel that the produced system fulfils this aim as all the functionality that would have been provided by these systems with a extra provided by the play/pause UI, is in the produced system.

My produced system achieves the aim set out in point 3. It uses intervals and the words typed in those intervals in order to form a profile of sorts for that user which is then compared against the data learnt for that user. However, in the produced system, the user is not able to modify the interval, this is something I would change if I were to make improvements to my program. I would allow the user to choose from a small subset of possible option as allowing the user to choose any time period could easily be abused by an imposter user by setting the period to a huge amount. Bar this, the produced system not only fulfils this aim but eclipses it with the produced system going far beyond this aim.

The final aim is also fulfilled. In my produced system, if the user fails validation, then the whole system is locked using the windows locked screen and the user is asked to re-authenticate thus fulfilling this aim.


\subsection{Time plans}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.27]{GantChartAll}
	\caption{Original Time Plans}
	\label{fig:GAll}
\end{figure}

Figure \ref{fig:GAll} showcases the original time plan for my project. This plan assumed I would rigidly stick to the ideas I had in my interim report. Due to the way, I altered my plans not only for the algorithms in use but also storage and added further functionality around testing, this plan was quickly modified. Several tasks were moved around with the mathematical basis being done after creating the keylogger. The login system and storage was scrapped entirely with the update functionality ad compression/decompression and play/pause functionality replacing it. Furthermore, the time allocated to the Wigner distribution was instead used to implement the validation measures and the classifier at the end of these methods. Overall, whilst I didn't stick to my plan all the time, it still provided a good basis for the project and ensured that I got all the work done and as such was crucial for the success of the project.

\section{Social, Sustainability, Commercial and Economic Context}

My program has potentially huge positive impact on people and businesses. The average cost of a cyber attack on a UK business in 2021 was £2760\cite{o'dea_2022}. My program is designed to improve the security of user's systems and as such installing the program for the business would help reduce this figure thus saving businesses money. My system is designed to stop imposters using a user's computer and performing malicious action, therefore installing the system on the user's computer would enable them to avoid this potential attack. My program would also go some way in reducing the number of cyber attacks worldwide which have seen a 15\% increase year on year.\cite{karafiloski_2021}. 

Unfortunately, my program could also have a negative impact on people and businesses. The underlying base of my project is a keylogger which can very easily be modified to be malicious. It could be used to record all users keystrokes including confidential matters like passwords and bank details. This would be a huge issue for both people and businesses and would cost them not only a lot of time but money as well. Furthermore, the data which the program collects could be used by advertisers to track them throughout the web and thus bombard them with ads. This is not really an issue for businesses but for people this could become an issue with data privacy. 

Overall, my system has many benefits to society. It will overall improve security of systems, reduce dependency on passwords and improve users trust in such security programs due to its play/pause functionality which stops the program from recording users confidential details. Furthermore, the highly accurate and lightweight nature of the program ensures that electricity consumption of the system will not be massively increased thus any negative sustainability impact is minimal. However, the program could have a negative impact on society due to the large amount of data it collects, the potential selling of users typing data and the easy way to make into a malicious keylogger. 

Keystroke dynamics authentication systems are at an early stage in their development. Not many such systems exist or are in use in the wider economy and many theoretical proposed systems are in development. One that I've found is called \emph{Typingdna ActiveLock}\cite{typingdna_2022}. This program claims that it does the same thing as my project. I believe it uses a machine learning approach however to learn how a user types. Systems such as this, I believe will become more widespread in the future and showcase the potential usecase in the wider economy.  The underlying maths is in use in a different kind of system however. \emph{TypingDNA focus}\cite{typingdna_2021} is a system which uses keystroke data in order to predict users moods and as such improve their productivity. Whilst the use case isn't exactly the same, the underlying process it. The program records keystrokes, processes the data and then makes a classification decision based on it. As shown by these systems, my system can already be used in commercial situations. However, in order to be properly suited for commercial uses, I would make a few small mainly cosmetic changes such as UI improvement, a better storage system and more robust code. This would enable the program to be more user friendly and secure and as such provide a better experience for the user.


\section{Personal Development}

Doing this project, enabled me to grow my skill base and as such improve my knowledge of many elements of the project life cycle. Whilst doing this project, I primarily learnt about data and how to manipulate it. I've learnt a lot about a number of different algorithms that I had no clue existed such as the Dynamic Time Warping algorithm in use in my project and the Wigners Distribution that I learnt about it in preparation for this project. I also learnt a lot about the use of git during the development life cycle. One example of how I've applied this new knowledge is my use of issues, branches and merge requests inside my Gitlab. This allowed me to develop new features whilst also safeguarding the work I've already done. Doing the project has also allowed me to further my knowledge of python which has allowed me to code in a more confident manner in the future. I've also developed further independent learning skills such as timekeeping due to the need to keep to deadlines. Doing this project whilst also juggling other university work and a part time job has ensured that I've come out of this project, not only with greater knowledge of   the algorithms and language I was using but also with better time-keeping skills and a more hard working nature.

\chapter{Conclusion}

Comparing the produced system and the planned system brings up a number of differences between the two. Whilst the produced system is more complex as a result of the increased features in play, it still is true to the original aims and objectives. For example, the aims stated the proposed system should be lightweight, accurate and secure and I feel that to a large extent this has been achieved. The aims also stated that the system should have the ability to form profiles for users which I feel as though my system does to an even higher standard than originally stated. The planned system should also have the ability to lock the user out which is inside the completed system. Overall, I believe that the system produced meets the original aims of the project and as such can be judged a success.

\bibliographystyle{ieeetr}
\bibliography{reference}
\end{document}
