\documentclass[10pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{wrapfig}
\usepackage{graphicx}
\graphicspath{{./Images/}}
\usepackage{listings}
\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
\pagenumbering{roman}

\title{Using Keystroke Dynamics to Authenticate a User Based on their Typing}
\author{Jack Francis}
\date{\today}


\begin{document}

\maketitle


\section{Abstract}

\tableofcontents

\chapter{Introduction}
\pagenumbering{arabic}
\setcounter{page}{1}



\chapter{Survey of Literature}

In order to create this project, it was necessary to do some research into the project. 

In order to do some research onto what a system based on keystroke authentication would involve, I read \emph{Time-frequency analysis of keystroke dynamics for user authentication}\cite{ToosiRamin2021Taok} which presented a highly theoretical one time system that used a maths based approach using a dynamic time warping algorithm along with a Wigner distribution in order to perform similarity calculations. The presented system only worked with small word samples. AUnfortuntly, the system wasn't a continuous system, but the paper still provided value in how to implement a validation measure. The validation function in my produced system is a highly modified version of the system set out in this paper. I decided to rely on this paper because it was published recently, well-cited, well researched and presented a solid approach I couldn't find any problems with.

Another paper I read in the opening stages of this project is \emph{Keystroke dynamics-based authentication service for cloud computing: Keystroke Authentication Service.}\cite{cloudComp} This paper showcased a continuous machine learning approach as opposed to the one time maths based approach showcased previously. Whilst I did not use the approach set out in this paper, the different viewpoint showcased was useful when making the decision in what approach to take. The reason I didn't end up choosing this approach because whilst the paper is relatively recent, it is not well-cited and the paper lacks detail in what is actually happening. Furthermore, I choose to go with a maths based approach because I felt this was more challenging. I decided I couldn't rely on this paper because of these reasons and as such it wasn't that useful to me.

Once I had made my decision in what approach to take, I read papers about algorithms used in certain steps of my validation function. The paper \emph{FastDTW: Toward Accurate Dynamic Time Warping in Linear Time and Space}\cite{Salvador2004FastDTWTA} presented the Dynamic Time Warping algorithm. The paper has been well-cited, presented the algorithm in great detail, was easy to understand and really helped me get an understanding of what the algorithm does. The paper is also peer reviewed and the authors well reputed. Therefore this paper was useful to me as it allowed me to get an understanding of one of the key algorithms in my project.

Whilst doing research on Dynamic Time Warping, I also read \emph{Fuzzy clustering of time series data using dynamic time warping distance}\cite{IzakianHesam2015Fcot} which used Dynamic Time Warping in order to improve their clustering algorithm. Whilst the  use case is not exactly the same as mine, the paper was still useful in order to see how they had prepared their data and what the results after going through the algorithm was. The paper is not well-cited but still proved to be useful due to it's subject matter and the clear approach stated within. This paper and the previous one allowed me to get a clear and deep understanding of how to implement and what the results are of the algorithm and as such are useful. 

I also did some research into the Wigners Distribution which I was planning on implementing inside the validation procedure. The paper \emph{Wigner Distribution}\cite{inbook} originally published in 2009 provided an exploration of this distribution on the context of quantum physics. Whilst the subject matter was different and it was difficult to understand to of the quantum maths elements which limited my understanding, I would still class this paper as useful. It allowed me to make a decision if this was strictly necessary to implement in my program and if the proposed algorithm provided enough accuracy to overcome it's performance hit. \emph{The Wigner Distribution: A Time-Frequency analysis tool}\cite{4766782} further increased my understanding of the distribution and proved useful in explaining details which were not explained by the previous papers.

Research into open-source examples was also undertaken with a look at a keystroke analysis system developed by Nikolai Janakiev\cite{janakiev_2018}. He created a theoretical one time system which uses machine learning in order to create models and work out how a user typed. This was useful as it was interesting to see how other people have approached the project. The approach detailed in there is based on the paper \emph{Comparing anomaly-detection algorithms for keystroke dynamics}\cite{5270346} which was of huge help when formulating my approach even though I didn't follow this approach.

The final paper I read was \emph{Keystroke Dynamics-Based Authentication Using Unique Keypad}\cite{s21062242} which showcased an approach using a unique keypad that the user uses to login. Whilst the keypad element isn't useful in my case, the approach they used to record and process the data from the keypad was useful and influenced my decision making when it came to the system. However, I didn't end up relying on this paper due to the highly theoretical nature of the system and it was not capable of being easily translated into my system.

\chapter{Design and Implementation}

I decided to create my system in python due to is flexibility, the large amount of documentation available, and my familiarity with it. Furthermore, python contains multiple packages such as NumPy, which makes doing advanced maths simple, easy, and provides excellent performance. Due to how my program operates, many data points are generated for each word. As such, having fast packages and function is essential to keeping the program as lightweight as possible and avoiding a potential performance impact on the user.

\begin{wrapfigure}{l}{0.5\textwidth}
	\begin{center}
		\includegraphics[width=0.48\textwidth]{OOP}
	\end{center}
	\caption{Basic Class Structure}
	\label{fig:ClassStruct}
\end{wrapfigure}

It also makes interacting with the various elements of my system simple due to a large number of packages available. I used an object-orientated approach in my system. Figure \ref{fig:ClassStruct} is a visual representation of the class structure used in my program. The main file is where the program starts and runs from. When the raw data is collected in the main file, it is passed into the Calculation class where the preprocessing, word forming, and validation procedures happen. Training inherits from the Calculation class and shares all methods and attributes with a saving to file function simply added on. The User profile is self-explanatory and just provides a simple way to store the user's details. The Word class contains details of each word produced by the program. It's a simple storage mechanism with many functions, such as generating the KD signal and compression attached among others.

\begin{figure}
	\includegraphics[scale=0.7]{OldPlan}
	\caption{Original System Plan}
	\label{fig:OldPlan}
\end{figure}

\begin{figure}
	\includegraphics[scale=0.55]{SystemPlan}
	\caption{Actual System Plan}
	\label{fig:NewPlan}
\end{figure}

My system is a relatively complex system that differs slightly from the one set out in my interim report. My plan from my interim report is shown in figure \ref{fig:OldPlan}. It's significantly more straightforward than my actual system plan shown in figure \ref{fig:NewPlan}. One main difference is the lack of a database to hold the user login information. Upon creating such a database, I felt this was unnecessary and, as such, omitted this from my final system in favour of using a Windows-based authentication system. This was done as too many authentication systems can confuse the user, and there is no better security than the windows one. A new feature in my final system plan is the play/pause UI which allows the user to pause the system should they do something involving sensitive information they do not want to be recorded. Another new step is the update step; if the user initially fails validation but then re-authenticates correctly using their windows username and password, the relevant word objects are updated. I went for a maths-based approach rather than using popular approaches such as machine learning because machine learning approaches typically have a lengthy learning phase that consumes a lot of the user's time and a lot of the user's system resources. Furthermore, a maths-based approach allows the system to be much more lightweight, and it's simpler to develop and for the user to understand what is going on. 

The entire system works at intervals. An interval is defined as a period in which the data is collected. For example, if the interval were 60 seconds, then data would be collected for 60 seconds, processed, and then the next interval would start. If the interval was shorter, then while the program would be theoretically more secure, it would suffer from a lack of data collected, which would affect the system's overall accuracy. Furthermore, having a shorter interval would lead to the program doing many more calculations, which would affect the user's system performance. Choosing an interval of 60 seconds allows me to balance accuracy and performance. While I tried other interval times such as 5, 30, and 45 seconds, these all suffered from a lack of data collected or started to affect performance severely. In particular, the 5-second interval never collected enough data to be able to make a call and sometimes took longer than five seconds to perform the calculations, which started to cause a problem due to the gaps in coverage. 

Each step is explained in more detail below. The user, upon first login, starts in the registration procedure. It is here that the system learns how the user types. It creates the profile data storage and then stores the data learned inside. The system then records the user's keystrokes for the interval. Once this has been completed, the data goes through the preprocessing steps and is then formed into words. For each interval, only a select few words are chosen for validation. Once the words are chosen, they are put through validation. This involves loading the learned data from the profile data storage and comparing the two using validation procedures. If they pass validation, the system moves on to the next interval and repeats the steps. If the words fail validation, the user is asked to re-authenticate using windows authentication UI. If they pass this, the failed words are updated in the validation procedure, and the system continues. If the user fails, a new user is created using the registration system. 

While all this is happening, the play/pause UI is running in a separate thread. Should the user ask the program to pause using this UI, they are asked to re-authenticate, and if this succeeds, the whole system is stopped until the user asks the program to continue again. 

The validation implementation is a rough version of one shown by Ramin Toosi and Mohammad Ali Akhaee in their excellent paper 'Time–frequency analysis of keystroke dynamics for user authentication'. \cite{ToosiRamin2021Taok} The paper is theoretical and describes an approach for performing validation on one word and then comparing them. It is, in essence, a one-time system, while mine is a continuous system that aims to keep the user safe. I have implemented their validation approach in my project while adding data gathering, word-forming, word selection, word storage, and update functionality. In figure \ref{fig:ValPlans}, you can see the two maths backends compared to one another. The main difference is that Wigners Distribution is omitted in the actual system. This is because this distribution is slow in my use case, and the improvements in accuracy are not worth the performance loss. The other difference between the two is the introduction of the Euclidean distance. This was introduced as a secondary validation measure to allow greater validation. This and the Correlation coefficient feed into the final similarity score.

The validation implementation is a rough version of one shown by Ramin Toosi and Mohammad Ali Akhaee in their excellent paper 'Time–frequency analysis of keystroke dynamics for user authentication'. \cite{ToosiRamin2021Taok} The paper is theoretical in nature and describes an approach for performing validation on one word and then comparing them. It is in essence a one-time system whilst mine is a continuous system that aims to keep the user safe. In my project, I've modified and implemented their validation approach whilst adding data gathering, word forming, word selection, word storage and update functionality. In figure \ref{fig:ValPlans} you can see the two maths backends compared to one another. The main difference between the two is that Wigners Distribution is omitted in the actual system. This is because this distribution is slow in my usecase and the improvements in accuracy are not worth the performance loss. The other difference between the two is the introduction of the Euclidean distance. This was introduced as a secondary validation measure in order to allow the greater validation. It's this and Correlation co efficient that feed into the final similarity score.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{MathsBackend}
		\caption{Interim report maths backend}
		\label{fig:IRMB}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{NewMaths}
		\caption{Actual System maths backend}
		\label{fig:ACMB}
	\end{subfigure}
	\caption{New and Old Validation Plans}
	\label{fig:ValPlans}
\end{figure}

\section{Data gathering and Forming}
My program relies on capturing the user's keystrokes, processing them and comparing them using a similarity measure. In order to do this, I decided to use the Keyboard Library\cite{boppreh_2016} as it is a lightweight, secure and modern library that makes capturing keystrokes easy. In my project, I use of the hook function of the library, which is used to "hook" onto a user's keyboard and record all the user's actions and create keyboard events for each action. The record function which makes use of this hook function is shown in figure \ref{fig:Record}. The code snippet is very simple; first the program will 'hook' onto the keyboard using the Keyboard Library mentioned above, record all keystrokes until the interval has passed and then stop recording. The start time of the interval and an array of Keyboard Events are then returned to the main body of the program. The start time of the interval is recorded and returned as it used further on in order to be able to place keyboard events on a time line in the context of the interval.

\begin{figure}[h!]
	\begin{lstlisting}
		def record(interval):
    		recorded = []
    		startTime = time.time()
    		keyBoardHook = keyboard.hook(recorded.append)
    		time.sleep(interval)
    		keyboard.unhook(keyBoardHook)
    		return recorded, startTime
	\end{lstlisting}
	\caption{Record Function}
	\label{fig:Record}
\end{figure}

A keyboard event is generated every time the user does something on the keyboard, whether pressing or releasing a key. Further information such as the type of the action (whether it was a press or a release), which key is this action happening on and a highly accurate time stamp of when the event occurred is also recorded. Figure \ref{fig:Hook}, shows an example of a keyboard event produced by the function when the user presses down the 'h' key.

\begin{figure}[h]
\centering
\includegraphics[scale=0.97]{KeyboardEvent}
\caption{The keyboard representation of a user pressing "h"}
\label{fig:Hook}
\end{figure}	

The first element in the array is the action, this can be either 'up' or 'down'. The next field is the scan-code, which I don't use but is useful for identifying keys. The next field is the name of the key which in this case is 'h'. After this is the time since the epoch in seconds, which is useful as this precise time-stamp is used to do the rest of the calculations. The other three fields are device, modifiers and whether the user uses a keypad. None of these I use in my program and are discarded almost immediately.

A small amount of pre-processing is then done on this data before it is paired up. The first step is to remove the scan code, keypad, modifier and device from each keyboard event and in order to make them lighter and more usable. The next step is to take the start time that is returned by the record function and subtract this from the time stamp in each keyboard event to get the time that the action occurred in the interval. Figure \ref{fig:preproc} shows what the data in \ref{fig:Hook} looks like after going through this.

\begin{wrapfigure}{l}{0.5\textwidth}
	\begin{center}
		\includegraphics[width=0.48\textwidth]{KeyboardEventPreProc}
	\end{center}
	\caption{Keyboard representation of a user pressing 'h' after pre-processing}
	\label{fig:preproc}
\end{wrapfigure}

The data collected at this point is stored as a 2D array with each sub array corresponding to an action. An example sub array is shown in \ref{fig:preproc}.

In this form, the data cannot be used for anything, as it currently takes the form of several individual actions that have no relation to one another. Therefore, the next step is to form pairs from the data. A pair is formed of one 'down' and one 'up' action where the key field matches and the 'down' is before the 'up'. This is done is that it allows the program to work with half as much data, which reduces the number of unnecessary data points and allows the program to be able to form words using these pairs.

\begin{wrapfigure}{r}{0.4\textwidth}
	\begin{center}
		\includegraphics[width=0.38\textwidth]{SimplePairing}
	\end{center}
	\caption{Example of simple pairing}
	\label{fig:SimpPair}
\end{wrapfigure}

There are two main rules to follow when pairing the data. In many cases, the user will press and release a key in quick succession without pressing any other keys. Due to the chronological nature of the data, pairing these types of presses is easy. All that is needs to be done is to iterate through the pairs and when we come to a 'down' action then simply select the next value in the array if it is an 'up' action and the key matches. Figure \ref{fig:SimpPair} shows an example of this type of pairing. However, this type of nice easy matching is not always the case.

In some cases, a user may press multiple keys down at once. For example, this might occur when the user is capitalising words using 'shift' or when the user is typing fast, so they may be already pressing down the next key before releasing the previous. An example of what the data will look like when this is the case is shown in Figure \ref{fig:WrongPair}. Applying the previous method in which we pair up keys with matching key types and opposite actions which are next to each other would result in the output shown in the pairs array. As you can see, this is not correct and would only lead to one pair where there should be two.

\begin{wrapfigure}{r}{0.4\textwidth}
	\begin{center}
		\includegraphics[width=0.38\textwidth]{PairingWrong}
	\end{center}
	\caption{Example of simple pairing}
	\label{fig:WrongPair}
\end{wrapfigure}

In order to fix this, it is necessary to include another case in the code. Suppose the current action does not have a matching key and opposite action next in the array. In that case, the program will iterate through the rest of the array starting from the current point, searching for the next entry with a matching key and an opposite action after the current. If it finds one it will pair them up. The key thing we assume for this to work is that every action has an opposite action. In nearly all cases, we assume this to be true as it is improbable that a user will hold down a key for the entire interval. If the method cannot find a resultant action event, then it is still added to the pairs list with an end time of the length of the interval.

The resulting pairing algorithm has a time complexity of \(O(n^2)\). In a program which is all about speed and minimal impact to the user, it is essential that the program has the lowest time complexity as possible. Due to the complicated nature of how users type, I believe this is the best time complexity for a problem of this nature.

\subsection{Forming Words}

After forming the pairs, the next step is to form words from these pairs. The words that the program forms are essential as it is this that the program uses to compare users. In English words take many forms, as such it is needed to account for numerous different possibilities in the word forming function.

A word is defined in my program as a sequence of pairs bounded by punctuation, white space or the use of modifiers such as 'shift'. In latter stages of this report, I refer to these as break pairs. The one notable exception to this rule is when an apostrophe or a hyphen is detected. If this occurs, then the program will check the previous pair and the pair afterwords and if both are letters and not numeric or punctuation, the pair including the punctuation is added to the current word. 

The data at this stage takes the form of a 2D array. The program will iterate through the 2D array it is given and check the key that the pair matches. If it is a letter or a number then it is added to another array which is used to store the current word being formed. If a break pair is found, it is not added to the current word, the current word is used to form a word object which is then saved to an output array. If the break pair is a white space pair, then the pair is simply skipped. However, if the break pair is a modifier such as 'shift' or 'ctrl' then the relevant entry in the semantics dictionary is updated for that user. This dictionary is used in the validation section of the project and is another indicator on how a user types. Backspace handling is done separately. If the user has pressed backspace, then the last letter added to the word is removed from it. The program can handle multiple backspaces even if they delete the entirety of the current word. If this occurs, the previous word object is popped off the array to be the current word and the last letter of this new current word is removed.

When the program gets to the last pair in the input array, if the pair is not a break pair then the pair is added to the current word and the current word forms a word object which is then saved to the output array.

The program will then return the output array which at this time is formed of word objects and the semantics dictionary. The output array is then saved to the wordsOut attribute of the Calculation class while the semantics is saved to the semantics attribute in the class.

\subsection{Data Selection}

If the program was to go through and check every single word for similarities, the cost in terms of time would be excessive and would make the program unfit for use especially if the user typed quickly during the interval. For example, if the program checked every word and the user typed 56 words in a 60-second interval, the time taken would be over 2 minutes as shown in figure \ref{fig:WordsvsTime} which while highly accurate and secure would render the program unusable as the time taken to process and perform all the similarity calculations would be in excess of the interval and as such would lead to a lower degree of accuracy and security. Furthermore, this would severely impact the performance of the user's computer and as such go against one of the main aims of the project.

As such, it is necessary to use a sampling method to choose words from the list of words chosen by the word forming function. While this is less accurate than checking every word, the performance gain over checking every word is huge with a large time saving per interval. Choosing how many words were selected was the next problem I endeavoured to fix.

I conducted a number of tests measuring how long the entire validation procedure took. Initially I started with 2 words chosen per interval with one chosen every at the start of the interval and one at the end. I then increased the number of words chosen by two each time with the interval remaining the same. At each testing point, the interval remained the same with a word selected evenly throughout the interval. Figure \ref{fig:WordsvsTime} shows the results of such a test. The time taken to perform the calculation increases linearly as the number of words chosen increases. If the word chosen is particularly long, then the time taken increases. The test data was the same for all tests with the user typing a paragraph containing 57 words of differing lengths.

\begin{figure}
	\centering
	\includegraphics[scale=0.70]{WordsChosenVsTime}
	\caption{Comparing words chosen to time taken in seconds}
	\label{fig:WordsvsTime}
\end{figure}

After performing the test, I settled on the program choosing four words per interval. Choosing four words took under 10 seconds and allowed the program to get coverage every quarter of the interval which is acceptable.

The process to choose the words is simple, it essentially will take one word from each quarter of the interval. This allows the program to get a good element of coverage.

\section{KD Signal}
Once the raw keystroke data has been formed into words and the words chosen, the next step is to transform the data from a word object made up of keystroke pairs into numerical data that can be used by later algorithms. The best way to do this is to transform the data into a measure of how many keys are being held down at a particular point in time. The resulting output is known as a key down signal (KDS). \cite{ToosiRamin2021Taok}

To convert a word into a key down signal, the start and end times of the word being transformed are used. Assume that \(w\) is the array of times that key actions occur in a particular word. \(w_1\) being the time of the first action and \(w_n\) being the time of the last action. This part will loop through all timestamps until it ends with the final time which is denoted by \(w_n\). The accuracy of this step is paramount as it is the level of detail that is the base accuracy for the rest of the steps. A higher accuracy means that the program will check more data points within this range at the cost of reduced performance. The current level of this is set to 4 decimal points which seems to provide a good balance between accuracy and performance.
\begin{equation}
\textit{KDS}(w) = \sum^{w_n}_{i=w_1}K(w_i)
\end{equation}
\(K\) is the next step of the algorithm and is heavily based on the KDS algorithm shown in \cite{ToosiRamin2021Taok}. \(n\) is the array of key presses that is used in the previous step. This step of the algorithm iterates through all the key presses and uses a modified Heaviside step function denoted by \(h\) which is run twice per pair with the time input from the previous denoted as \(t\) and the 'down' action denoted as \(n_i^1\) and the 'up' denoted as \(n_i^2\). The value returned by the Heaviside step function with the 'up' action is subtracted from the value returned by the 'down' function.
\begin{equation}
\textit{K}(t) = \sum^{n_k}_{x=1}h(t,n_i^1)-h(t,n_i^2)
\end{equation}
The reason for this subtraction is that the purpose of this measure is to return the number of keys pressed down at the time input. Once a key has been released it is essential that the key is removed from the measure. For example, if a pair exists with down action time being at 1 second and up action time being 1.1 seconds. At time, 1.5 the equation will equal \(1-1=0\). However, if the time put in is 1.05 then the equation will be \(1-0=1\) which indicates that one key was being held down at this particular time.

For every time input, each pair is checked with the sum of all the results stored in a dictionary along with the time input as the key. It's this dictionary that forms the KD signal and is used in further steps.

\subsection{Heaviside Step Function}
This is the bottom layer of the KD signal algorithm. It is a modified version of the Heaviside Step function.
\begin{equation}
	h(x_1, x_2) = \begin{cases}
	1 & \text{ if } x_1 > x_2 \\
	0.5 & \text{ if } x_1 == x_2 \\
	0 & \text{ if } x_1 < x_2
\end{cases}
\end{equation}
The modification done is very simple, the only change is the addition of a third case which tests if the two times are equal to one another. Due to the nature of the use case for my project, there is a relatively high chance that the two times are equal to one another. In this case this means that the user at this time is currently in the process of performing that action whether that be pressing or releasing the key. The theoretical basis for this function is taken from "Time–frequency analysis of keystroke dynamics for user authentication" by Ramin Toosi and Mohammad Ali Akhaee\cite{ToosiRamin2021Taok}

\subsection{Output}
The resulting signal can be shown easily in graph format. Figure \ref{fig:KDS1} and figure \ref{fig:KDS2} show the same user typing the same word twice in two different intervals. Figure \ref{fig:KDSBoth} is both sets of data overlaid on the same graph. As you can see, even with the same user typing it, both sets look very different and normalisation becomes essential in order to be able to compare the data accurately. 

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{KDS1}
		\caption{First user typing hello}
		\label{fig:KDS1}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{KDS2}
		\caption{Second user typing}
		\label{fig:KDS2}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{KDSBoth}
		\caption{Both signals overlayed on each other}
		\label{fig:KDSBoth}
	\end{subfigure}
	\caption{KD Signals generated by a user typing hello}
	\label{fig:KDS}
\end{figure}

\section{Dynamic Time Warping}

The data in my project requires normalising to be useful. The algorithm that I chose to use is called the Dynamic Time Warping algorithm. Two sets of data, one loaded in from the word file and the other generated from that interval is input into this function in order to normalise it. As both sets of data are input, the Dynamic Time Warping algorithm attempts to change both sets of data in order to make them as close as possible. The output is two sets of normalised data which can then be compared against one another to provide a similarity score. 

In my program, I chose to use the fastdtw library \cite{tandi_2015}. Like the name suggests, this library provides a really fast implementation of the dynamic time warping algorithm with the algorithm taking only \(O(n)\) time complexity\cite{tandi_2015}.

The output of the two datasets shown in figure \ref{fig:KDS1} and figure \ref{fig:KDS2} both put through dynamic time warping is shown in figure \ref{fig:KDSDTW}.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{KDS1DTW}
		\caption{First after DTW}
		\label{fig:KDS1DTW}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{KDS2DTW}
		\caption{Second after DTW}
		\label{fig:KDS2DTW}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{KDSBothDTW}
		\caption{Both signals overlayed on each other}
		\label{fig:KDSBothDTW}
	\end{subfigure}
	\caption{KD signals after going through DTW}
	\label{fig:KDSDTW}
\end{figure}

Shown in figures \ref{fig:KDS1DTW} and \ref{fig:KDS2DTW} is the results of the data in figures \ref{fig:KDS1} and \ref{fig:KDS2} after going through dynamic time warping. Despite going through the process, you can see that they haven't changed much and look the same. This is because both signals were typed by the same user and as such are very similar. However, the first signal has an extra datapoint on the end which causes it to go on longer than the other signal. This is not a huge issue however, as the validation systems explained below returned the values of \(1.0\) and \(0.99999\) which showcases that both signals are the same.

\subsection{Path}
The implementation i chose to use returns two values, the distance and the path. The distance is used in the validation step to calculate euclidean distance. The path is more important however. It is what is used to "warp" the data. Theoretically, it is the least costly path through a cost matrix. It essentially is a pairs of indexes in the two datasets which map up. It states which values need to go into the datasets. 

\section{Validation Measures}

Finally the data is in a state where we can perform validation. In order to improve the accuracy of the program, I use two different validation methods along with the semantics collected in the word forming stage. 

The two methods I use along with the semantics are euclidean distance which is done in the dynamic time warping stage. This has secondary importance compared to the 2D correlation co-efficient. The reason for this, is that the euclidean distance is simply a measure of the distance between two points and is not a good indicator for data such as this. Therefore, I use this as a rough figure and use the correlation co-efficient as the main method. 

The values produced by these validation methods are then put together to form a value between 0 and 100. This is then compared against the confidence level. The confidence level is a integer which determines how similar the data loaded in from the file and the data collected in the interval have to be. For example, if the validation methods in the interval (Similarity Score) return a value of 0.75 and the confidence level is 0.8, then the user is rejected and asked to re-authenticate. When developing this project, I settled on a confidence level of 0.8.  Only the euclidean distance and the 2D correlation go into calculate the similarity score. The semantics are used to either raise the confidence level or lower it depending on the value returned by the semantics validation.

\begin{wrapfigure}{r}{0.4\textwidth}
	\begin{center}
		\includegraphics[width=0.38\textwidth]{SemanticsEx}
	\end{center}
	\caption{Example of a users semantics file}
	\label{fig:sem}
\end{wrapfigure}

The semantics validation is very simple. In the words step of the validation procedure, the semantics of the data input is collected and stored in the class. Each user will also have a semantics file stored in their data directory. An example of a users semantics file is shown in figure \ref{fig:sem}. The data stored in the file and the data collected for that interval are then compared against one another. If for example, the user in the past has used the shift key and in this interval has also done so, then the confidence level is lowered by 0.02. However, if the user in the past has always used the caps lock key and never shift then they don't match and the confidence level is raised by 0.02. The reason of such a small change in the confidence level is that users habits regarding this are not typically set in stone and as such any large change will likely lead to reduced accuracy.

The validation methods are explained further in sections 3.4.1 and 3.4.2 respectively. Here I will outline the entire validation process. The flow chart in figure \ref{fig:valProc} provides a visual representation of the first part of the validation procedure.

\begin{wrapfigure}{l}{0.4\textwidth}
	\begin{center}
		\includegraphics[width=0.38\textwidth]{DistancesDict}
	\end{center}
	\caption{Example of a distances dictionary after validation calculations done}
	\label{fig:distances}
\end{wrapfigure}

\begin{figure}
	\centering
	\includegraphics[scale=0.3]{ValidationProc}
	\caption{Validation Procedure}
	\label{fig:valProc}
\end{figure}

Figure \ref{fig:valProc} explains the validation procedure.

As shown in figure \ref{fig:valProc}, the words chosen in the first step are iterated through. If a word already has a word file stored in the users directory then that is loaded in and decompressed. Otherwise, the procedure just returns two None values. The two data sets, one loaded in and the other generated from the current interval are then both put through Dynamic Time Warping where the euclidean distance is calculated and stored. The path generated by the DTW is then used to 'warp' both datasets. It is at this stage that the 2D correlation co-efficient is used in order to calculate one element of the similarity score. The Euclidean distance and the correlation co-efficient are then stored in a dictionary with the key being the index in the chosen array. Figure \ref{fig:distances} showcases an example distances dictionary. In the example, the first two elements were typed by a genuine user with the third being typed by an imposter and the last word has not been seen before so the validation procedures just returned None.

After the distances dictionary has been formed with all values inside. The next step is to categorise the values. First of all we have to define the confidence level for each validation procedure. As shown in the flowchart (\ref{fig:valProc}), these are adjusted based on the results of the semantics validation. The distances dictionary is iterated through and each value checked. Figure \ref{fig:DistanceTransform} below shows what happens in each individual case where \(e\) is the Euclidean Distance, \(c\) is the value returned by the correlation co-efficient, \(i\) is the confidence level for the correlation co-efficient and \(b\) is the confidence level for the Euclidean Distance.

\begin{equation}
	\begin{cases}
		\textit{None} & e == \textit{None or } c == \textit{None} \\
		\textit{True} & c >= i \textit{ and } e <= b \\
		\textit{True} & c >= i \textit{ and } e >= b \\
		\textit{False} & \textit{Otherwise}
	\end{cases}
	\label{fig:DistanceTransform}
\end{equation}

The value returned by this function is then adding into an array which is used in the next step. Figure \ref{fig:wordCheck} shows what resulting array formed by putting the data in \ref{fig:distances} through this part of the validation procedure. This example presumes that the confidence levels for Euclidean Distance and correlation co-efficient are 1000 and 0.84 respectively.

\begin{figure}[h]
	\centering
	\includegraphics{WordCheck}
	\caption{The result of the data in figure \ref{fig:distances} after going through further validation}
	\label{fig:wordCheck}
\end{figure}

Transforming the data this way allows the order of the data to be preserved. Furthermore, it allows the next stage of the validation to simplified heavily as rather than working with variable data, we know to need to only consider whether the value is one of True, False or None which vastly simplifies the complicated system used to make a decision which is explained in the next step.

The "wordCheck" array shown in \ref{fig:wordCheck} is then used in the next step in order to finally perform an action based on the data. Despite what happens next only three outcome occur. Figure \ref{fig:If1} showcases what happens if only one word is in chosen and an output returned to it. If the word is true, then the person checks out and the program moves on to the next interval. However, if the word doesn't pass the validation checks or has never been seen before, then further processing has to be done. 

This is very complex. If the user fails validation then their pc is locked and they are forced to re-authenticate. In order to perform this action, I load in the default python library sub process \cite{subprocess} which I then use to call the windows lock dll file. This enables me to lock the user out of the computer easily and efficiently without much wasted code or overhead in terms of performance. If the user has changed when the user does re-authenticate, then 'New' is returned. This tells my main body of the program that it is a new user and as such a new keyboard is created for that user. If the opposite happens and the user is re-authenticated successfully, then the word files and semantics are updated and the program returns true.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{IfLen1}
	\caption{Flowchart which illustrates decision tree if only one word is chosen}
	\label{fig:If1}
\end{figure}

The validation function can take in as a parameter a 'mode'. This determines what happens should validation fail. The default mode is 'r' which stands for real. This allows the program to do the entire process including locking the users computer if validation fails. 'rnl' stands for real no lock which is mainly used for testing and demonstration purposes because it stops the users pc locking and just returns false. A 't' flag also exists, this indicates that the program is in test mode and when in test mode the program will simply save every word.

Typically the user will type multiple words in an interval. As the result outputted above from the wordCheck can take any one of three values for each word, there are plenty of variations. These are all explained below.

Possibility 1: All words pass Validation and wordCheck consists of just True values
True and an empty array are returned. The runner which runs the validation process then continues on to the next interval.

Possibility 2: Some words pass validation and some words have never been seen before.
This results in the program creating word files for those which have not been seen for using the update function which is detailed in \ref{sssec:update}. True and an empty array is then returned and the program as a whole moves onto the next interval.

Possibility 3: All words have never been seen before or fail validation.

When a combination of never seen before and failing validation is detected, then the program has no confidence that the user is genuine, therefore it starts the re-authentication procedure which is explained above.

Possibility 4: The chosen only consists of words which the program has never been seen before.

Much like possibility 3, the program has no way of knowing whether the user is genuine or not. Therefore it takes the safest course of action and forces the user to re-authenticate. It then does the exact same as possibility 3 does.

Possibility 5: All words fail validation
If all words fail validation, then the program is confident that an imposter is attempting to use the computer. The function locks the pc and demands re-authentication from the user. Like in possibility 3 and 4, it will update the words that fail validation if the user passes re-authentication.

\subsection{Euclidean Distance}

The first validation method used is the Euclidean Distance. It is a system of measure between two data points. In my system it is used on the warped data in order to generate the distance between them. The overall score is calculated by adding up all the distances between the data points output by the KD signal function and the data points input from the word file. This is results in a very good benchmark figure to determine how close the two signals are together. 

\begin{equation}
\textit{euc}(a,b) = \sqrt{\Sigma^n_{i=1}(a_i-b_i)^2}
\label{fig:EUC}
\end{equation}

Figure \ref{fig:EUC} shows the mathematical formula for euclidean distance where \(a\) and \(b\) are the two KD signals, and \(n\) is number of data points. This then returns a value like we can see in the first element of the array in \ref{fig:VCompare}. The value returned is self-explanatory with a smaller value indicating less difference between signals than a large one. In my program rather than calculate this value myself, I used scipys implementation of this function\cite{scipycommunity_2022}. TAttempting to do this myself in pure python resulted in slow processing times. The function used by scipy is much faster due to it being written in C++ which is far better at doing these functions with large datasets produced by the KD signal function.

\subsection{Correlation Coefficient}

\begin{wrapfigure}{l}{0.4\textwidth}
	\begin{center}
		\includegraphics[width=0.38\textwidth]{EucVs2D}
	\end{center}
	\caption{Euclidean Distance compared to 2D correlation co-efficient}
	\label{fig:VCompare}
\end{wrapfigure}

2D correlation co-efficient is used because it is relatively lightweight, quick, easy to implement and provides easy to interpret results. As opposed to the value produced by the euclidean distance, the output returned by the 2D correlation co-efficient can only take a range of values between -1 and 1. If the function returns 1, then the two signals input are exactly the same, 0 if they are radically different and -1 if they are the same again. An example of the output returned can be seen in the second element of the dictionary in figure \ref{fig:distances}. In shortened form, the 2D correlation figure essentially returns a percentage of how close the two input signals are.

This measure is used after the dynamic time warping and take the two input signals as inputs. This is weighted higher in my system due to the stronger accuracy and ease of use. One example of why the this is more accurate than euclidean distance is shown in the figure \ref{fig:VCompare}. 2D correlation co-efficient returns around 0.95 for both, whist euclidean distance returns 180 and 300 which are wildly differing figures.
\begin{equation}
	f(a,b) = \frac{\Sigma^n_{i=1}(a_i-\overline{a})(b_i - \overline{b})}{\sqrt{\Sigma^n_{i=1} (a_i-\overline{a})^2}\sqrt{\Sigma^n_{i=1}(b_i-\overline{b})^2}}
	\label{fig:corrco}
\end{equation}

Figure \ref{fig:corrco} shows the equation that my implementation is based upon where a and b are the two warped arrays passed in, \(n\) is the length of either array, \(a_i\) and \(b_i\) are the values at positions \(i\) in each array respectively. My code implementation is shown in figure \ref{fig:CodeCorr}.

\begin{figure}[h!]
	\begin{lstlisting}
		# Correlation Coefficient
        cov = 0
        XSum = 0
        YSum = 0
        Xmean = sum(ff_warped)/len(ff_warped)
        Ymean = sum(ii_warped)/len(ii_warped)
                    
       	for i in range(len(ff_warped)):
        	cov += (ff_warped[i] - Xmean)*(ii_warped[i] - Ymean)
            XSum += math.pow(ff_warped[i]-Xmean, 2)
            YSum += math.pow(ii_warped[i]-Ymean, 2)
                            
        correlationCoefficient = cov/((math.sqrt(XSum)*(math.sqrt(YSum))))
       \end{lstlisting}
       \caption{Code Implementation of correlation co-efficient}
       \label{fig:CodeCorr}
\end{figure}

The code is self-explanatory, with the Xmean and Ymean variables simply calculating the mean for the two input arrays respectively. The output of this function of this is then stored along with the euclidean distance in a dictionary to be used for further processing.

\section{Training}
When a new user is created, it is essential that training happens. Otherwise, the program has nothing to go on and as such locks the computer at every interval. This is incredibly annoying for the user but also the learning process would be slow. Therefore it is necessary to add a training phase. There are two potential approaches to a training phase. The first is to have a dedicated text that the user types out and the second is that for the first \(x\) intervals all word data is saved and then used as a bank. I explored both of these attempts in my project.

The first one I explored was the dedicated training phase with a dedicated sample text that the user types out. The difficulty with this wasn't making the UI or ensuring that the data was captured, it was coming up with the text itself. In order for this to work properly, a text was needed that contained all of the most common words that a user uses along with a wide range of  punctuation. This in itself is difficult as each user is different. My program currently use the text shown below as the dedicated text.

"Since they are still preserved in the rocks for us to see, they must have been formed quite recently, that is geologically speaking?
What can explain these striations and their common orientation?			
Did you ever hear about the Great Ice Age or the Pleistocene Epoch?			
Less than one million years ago	in fact	some 12000 years ago an ice sheet many thousands of feet thick rode over Burke Mountain in a southeastward direction.
The many boulders frozen to the underside of the ice sheet tended to scratch the rocks over which they rode.			
The scratches or striations seen in the park rocks were caused by these attached boulders.				
The ice sheet also plucked and rounded Burke Mountain into the shape it possesses today."

This piece of text was chosen as it was long enough to get a wide variety of punctuation in but so long that the user gets bored. A wide variety of words is also used in order to attempt to get the best coverage available.

The next approach I tested was having the first 5 intervals for a new user simply recording and saving all words that are used. This gives better coverage than the other approach because it allows the program to learn the users habits and common words. This approach is also deals well with different users and allows greater insight into the user. While this method is good at learning users, it does potentially open up a security issue. If all words, are simply being saved and stored, nothing stops an imposter user from altering this training data. This approach has another disadvantage, it doesn't learn how the user uses punctuation which the other approach does. 

Whilst exploring both of these, I came to the conclusion that a mixture of both training methods would be the best approach. This would ensure that the program gets the best of both worlds. Therefore, I decided to use both systems with the first 5 intervals after the user finishes training being the training intervals. The reason 5 was chosen as the training intervals is because it minimises the security risk caused by an imposter user whilst still proving to be an effective data gathering tool.

\begin{figure}
	\begin{lstlisting}
		def runner(id, prof, stop):
    		count = 0
    		while True:
        		data, start = record(interval)
        		if stop():
            		break
        		if trainingItersYN == False or count > trainingIters:
            		inter = i.Calculation(data, start, prof, 1)
            		if stop():
                		break
            		decision, index = inter.validation(mode='r')
           			if decision == False:
                		break
        		else:
           			if count <= trainingIters and len(data) != 0:
                		inter = t.Training(data, start, prof, 1,0)
                		count += 1
                		if stop():
                   			break
      \end{lstlisting}
      \caption{The main runner function with training intervals in}
      \label{fig:runner}
\end{figure}

The code in \ref{fig:runner} highlights the main "runner" code. This takes the data in from the record function, if training iterations is turned on and the number of training iterations has been exceeded, then the validation function is run with that data and a decision returned. If that happens the pc goes through the authentication specified by the validation procedure. However, if the opposite is true and it is a training interval then the data is simply saved as specified by the training class which simply saves the KD signal generated for that word.

\section{Update}
\label{sssec:update}

The way users type change over time. This is caused by a number of factors such as age or change in desk setup. For example, the way a user types at home will be different to the way users type at a coffee shop. Therefore, it is important that my program can learn and adapt. This is where the update function comes in. This step is done after the validation step and is called in possibility where a user has failed validation but re-authenticated successfully. Essentially this is called where the program has got it wrong. 

The function is very simple. It will iterate through all words that have failed validation and simply update the word files with the data collected from the current interval.

\section{Storage}
\label{sssec:Storage}
In order for this project to work correctly, it is essential that the program has a way of storing how users type certain words. It is necessary to build up a storage of the words that the user has typed in the past. In my project this is done by storing the KDS generated in the KD step and storing it in a json file. The reason for storing it this instead of storing it inside a database is that the data is highly variable and as such it is not possible to store the data inside a database. For example, one signal might have 90,000 datapoitns while another might only have 5000. As such, it's not practical to store this kind of highly variable data inside a database. Each file is named after the word contained and represented by the KD signal inside. For example, if the word was "hello" the file would be named hello.json. Storing it this way ensured easy access to the data within and simple identification of which data belonged to which individual. Storing just the KD signal as opposed to storing the pairs or the raw keystroke data collected in the first place has many advantages. The first major advantage in storing just the KDS is that a malicious user cannot easily read how a user types just by looking at the files. Secondly, this type of data is easy to read by the program. Thirdly it stops the program having to perform the KDS element of the validation procedure every time is loads in a file resulting in less performance impact. Simply loading in and comparing is far easier than processing all of the data again and then comparing it. Figure \ref{fig:uncomp} shows a small excerpt of one of these word files.

The main problem with storing the data this way is that file sizes can become a problem. Generating a KD signal generates a lot of data points and when these are all stored, the size of each individual word file starts to become an issue. Users also tend to use a lot of words and once the user has been using the program long enough, the size of the users directory can become huge. One way to fix this would be to reduce the amount of data points being generated for each word. Unfortunately this isn't feasible as accuracy is negatively impacted. Therefore the solution it to compress the data points when they are saved to the file and then decompress them when they are loaded back in.

\begin{wrapfigure}{r}{0.4\textwidth}
	\begin{center}
		\includegraphics[width=0.38\textwidth]{UncompressedData}
	\end{center}
	\caption{A sample of an uncompressed word storage file}
	\label{fig:uncomp}
\end{wrapfigure}

Figure \ref{fig:uncomp} shows a sample of the representation of the data stored without compression whilst figure \ref{fig:compression} shows the same data after compression. Due to the way the KD signal is generated, there are a large amount of duplicate values. My compression algorithm works by storing the range in values at which the KD signal is the same. In figure \ref{fig:compression} you can quiet clearly see that between the timestamps of 1.5649 and 1.6368 the value did not change from 1. Previously all of the timestamps between these two values would have been stored leading to a bloated file size that contained a lot of unnecessary information.

This type of compression makes a huge impact on the file sizes. For example, the file storing the data in figure \ref{fig:uncomp} without compression is approximately 91 kilobytes whilst the compressed version shown in \ref{fig:compression} is approximately 735 bytes. This is a large saving in terms of space, with the compressed version being 99\% smaller than the compressed version. Whilst 91 kilobytes may not seem like a lot, when the average office worker types at approximately 30 words per minute\cite{naskar_2020} for approximately 8 hours a day leading to around 14400 words typed per day. If even a small percentage such as 10\% of these words are unique, then the resulting uncompressed word data would take up 131040 kilobytes or 131.4 megabytes worth of space. However, if the data is compressed then the same amount of words would only take up 1.05 megabytes. In this particular case this results in savings of 130 megabytes which is a huge amount.

\begin{figure}[h!]
	\begin{lstlisting}
[
    {"key": [1.5647, 1.5647], "value": 0.0}, 
    {"key": [1.5648, 1.5648], "value": 0.5}, 
    {"key": [1.5649, 1.6368], "value": 1.0}, 
    {"key": [1.6369, 1.6369], "value": 0.5},
    {"key": [1.637, 1.664], "value": 0.0}, 
    {"key": [1.6641, 1.6641], "value": 0.5}, 
    {"key": [1.6642, 1.772], "value": 1.0}, 
    {"key": [1.7721, 1.7721], "value": 0.5}, 
    {"key": [1.7722, 1.7955], "value": 0.0}, 
    {"key": [1.7956, 1.7956], "value": 0.5}, 
    {"key": [1.7957, 1.865], "value": 1.0},
    {"key": [1.8651, 1.8651], "value": 0.5}, 
    {"key": [1.8652, 1.9731], "value": 0.0}, 
    {"key": [1.9732, 1.9732], "value": 0.5}, 
    {"key": [1.9733, 2.0206], "value": 1.0}, 
    {"key": [2.0207, 2.0207], "value": 0.5}, 
    {"key": [2.0208, 2.1287], "value": 0.0}, 
    {"key": [2.1288, 2.1288], "value": 0.5}, 
    {"key": [2.1289, 2.2002], "value": 1.0}
]
	\end{lstlisting}
	\caption{KD representation of a user typing hello after compression}
	\label{fig:compression}
\end{figure}

Performance is important to the success of the program but reducing this slightly in order to save storage space on the users computer is acceptable. The next step is where to store these word files.

Each user has a "User Profile". This essentially stores information about the current user such as the profile name, their current keyboard, whether they are a new user or not and how many keyboards they have. When a user profile is created as a result of either a new profile being created in the training procedure or "New" being returned from the validation procedure, a new folder is created in the Data folder inside the programs home directory which is named after the users windows user name. The python library has a built in function called "getpass" which simply interfaces with windows in order to get the current users information and provide a secure interface for password input\cite{getuser}. In order to get the current users information, I use the getuser function from this library. Each keyboard that the user has is stored inside the users folder. The users word information is stored inside the relevant keyboard along with their semantics storage file. A typical folder structure is shown in figure \ref{fig:foldStruct}.

The reason it is done this way is that users type differently on different keyboards and it made sense to have a way to store all the different profiles that the user may have. The semantics file is also stored inside the keyboard folder. This is for the reason that users may use caps lock on one keyboard and shift on another. It ensures that the program can really narrow down for certain users and really learn the user. The user profile class stores the list of keyboards along with the path to the current keyboard.

\begin{wrapfigure}{l}{0.3\textwidth}
	\begin{center}
		\includegraphics[width=0.28\textwidth]{FolderStruct}
	\end{center}
	\caption{Example of folder structure after light usage by a user with one keyboard}
	\label{fig:foldStruct}
\end{wrapfigure}

\section{Pausing}

One main problem with my system was the lack of user privacy. For example, a user wouldn't want the program to pick up and store details about their bank password. Therefore, it becomes necessary to introduce a pause functionality. This is very simple and consists of a simple user interface screen which consists of one button which is shaped like a pause button. When the button is pressed the pause symbol changes to a play and the system stops recording the users keystrokes until the button is pressed again. The UI that powers this functionality runs on the main thread whilst the calculation happens on another thread. When the button is pressed to pause the calculation, the stop thread flag is switched to True which stops the thread and causes it to return. As such, when the calculation is switched on again a new thread is created and the calculation continues.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{PauseEx}
		\caption{Pause UI}
		\label{fig:pauseEx}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{PlayEx}
		\caption{Play UI}
		\label{fig:playEx}
	\end{subfigure}
	\caption{Example of play/pause UI in both states}
	\label{fig:PPUI}
\end{figure}

However, this system is open to abuse because an imposter could simply switch the system off, use the system and then switch is back on. In order to combat this, when the user decides to pause the program, they are prompted to re-authenticate using the windows login system. This essentially combats this problem and ensures that data collection remains. Figure \ref{fig:PPUI} showcase both states of this simple section of the UI. 

Figure \ref{fig:stoppingThread} showcases the function that is run everytime the button is pressed. It essentially checks what image is being displayed. If the pause image is being displayed, then the program calls the windows UI in order to authenticate the user. If they pass this, then the image is changed, the stop threads flag is switched to true and all threads stop. If the play image is being displayed, then the image is switched and the calculation sub-process is started back up again.

\begin{figure}
	\begin{lstlisting}
	def stop():
    global stop_threads
    if button.cget('image') == 'pyimage2':
        cmd='rundll32.exe user32.dll, LockWorkStation'
        subprocess.call(cmd)
        button.configure(image=imgPlay)
        button.image = imgPlay
        stop_threads = True
        for worker in workers:
            worker.join()
    else:
        button.configure(image=imgPause)
        button.image = imgPause
        stop_threads = False
        tmp = threading.Thread(target=runner, args=(0, prof, lambda: stop_threads))
        tmp.start()
        \end{lstlisting}
        \caption{Code for controlling calculation threads}
        \label{fig:stoppingThread}
\end{figure}

\chapter{Results and Discussion}

In order to be able to test the accuracy of my system it is necessary to first define some measures. False Positive (FP) is used to define the case when the system lets in an imposter user. False Negative (FN) is the opposite of this where the system rejects a user it shouldn't have. True Positive (TP) is where the system makes the correct decision and allows in a user correctly. True Negative (TN) is where the system makes the correct decision and rejects a user where it should have. Hold time (HT) is the time in which a key is held down or the time between the when a key is pressed and when a key is released. The float time (FT) is the time between actions. It can be defined as the time between the user releasing a key and pressing down the next key.

In order to test the system, it was necessary to create some data. In order to do this, I made a function which would generate test data with timings I specified. This enables me to generate lots of test data simply and easily. Furthermore, it allows reproducibility in the data collected. If the data was simply a user such as myself typing, it is almost impossible to get the same timings in terms of typing more than once. This function can be seen in Appendix C. That function can only deal with data that is linear (where a 'up' action is always followed by a 'down' action).


\section{Test 1: Validation Measures}

The first tests I conducted on the system consisted of checking if results returned by the validation procedures are what is expected. In this test, the confidence level is irrelevant as we are not measuring this. The test data consists a singular word "geographically". The first test in this set of tests consisted of me generating 10 sets of test data using the same word each time. Each set of test data increased the hold time by 0.1 whilst keeping the float time the same.. The program was in 'rnl' which ensured that it did not lock the pc or update the data. This was done in order to ensure that all tests were tested against the same dataset. The reference dataset which was loaded in each time consisted of the 5th set of test data. This test should return an inverted bell-shaped curve due to the reference data point being in the middle. 

\begin{figure}
	\centering
	\includegraphics[scale=0.6]{EUCGraph}
	\caption{Euclidean Score}
	\label{fig:EucGraph}
\end{figure}

Figure \ref{fig:EucGraph} shows the results of the testing. As shown, the test data results in a inverted bell curve which is expected. The smallest euclidean distances being in the middle as these are the most similar to the reference data that all datapoints are checked against. This proves that the euclidean distance is working as expected. Interestingly, the euclidean distance score is not a perfect inverted curve due much higher results on the left side of the graph rather than the right. I'm not too sure why this happens but I believe it may be because of the way in which Euclidean distance is calculated. It's because of results like this that the euclidean score is used as a estimation and a secondary measure to the 2D correlation co-efficient.

\begin{figure}
	\centering
	\includegraphics[scale=0.6]{CorrGraph}
	\caption{2D Correlation Co-efficient Score}
	\label{fig:2DGraph}
\end{figure}

Figure \ref{fig:2DGraph} graphs the hold time against the 2D correlation score. Similar to the graph shown in figure \ref{fig:EucGraph} it takes the shape of a bell shaped curve. Unlike the euclidean graph this is a far more uniform graph which highlights once again that the 2D correlation co-efficient is a better measure than euclidean distance. The table below shows the raw data of such a test.

\begin{center}
	\begin{tabular}{|c|c|c|c|}
	\hline
	HT & Euc & Corr & Dec \\ [0.5ex]
	\hline
	\hline
	0.1 & 13997 & 0.56 & False \\
	0.2 & 6001 & 0.72 & False \\
	0.3 & 6001 & 0.73 & False \\
	0.4 & 1.5 & 0.99 & True \\
	0.5 & 1.5 & 0.99 & True \\
	0.6 & 1.5 & 0.99 & True \\
	0.7 & 1.5 & 0.99 & True \\
	0.8 & 6000 & 0.74 & False \\
	0.9 & 4000 & 0.83 & False \\
	1.0 & 10000 & 0.58 & False \\
	\hline
	\end{tabular}
\end{center}

The raw data points to another interesting anomaly in the data, \(0.9\) if following the trend should have a euclidean distance greater than in \(0.8\) and a 2D correlation co-efficient less than in \(0.8\). However, this is not the case. This small anomaly doesn't affect the system in a huge way. Calculating the accuracy of this data we can see that 9/10 data sets follow the trend leading to an accuracy of 90%.

\section{Test 2: Decisions}

The next logical test to do is to test whether the decision being made. This is where the measures defined in the first paragraph of this section come into use. This test essentially tests whether the system can tell the difference between a genuine and an imposter user. In order to do this, I generated and saved some test data. Due to my program only choosing four words from an interval, each interval is comprised of only four words that are the same for each test. Any more and the program would not be able to test each interval correctly. No data is updated in the intervals and the sets of data are tested against a genuine sample of the users data. Different combinations are stored in these intervals. For example, in the first interval, all four words might be genuine whereas in the next interval, there might be three imposter words and one genuine. All possibilities are tested and written below. A copy of the data is stored inside the gitlab under test/data.

The first possibility is that one word in the four is an imposter user




\begin{itemize}
	\item Test Results
	\item Calc and use FP, FN, TP, TN - get a percentage
	\item Discuss in relation to validation measure
	\item Mention struggling with small words maybe???
	\item Speed, security??
\end{itemize}


\chapter{Critical Appraisal}




\section{Summary and Critical Analysis}

 Whilst the produced system differs from the word explained in the outline as shown by figures \ref{fig:OldPlan} and \ref{fig:NewPlan}, the functionality remains the same. I would go as far as to say that the produced system is an upgrade on the proposed system. The produced system is accurate, lightweight, secure and as such fulfils all of the original aims. Whilst some functionality around login has not been implemented due to not being needed, the expanded functionality added far outweighs this. The expanded functionality being the compression and decompression function which vastly reduce the amount of storage space being used in the user's computer, the update functionality which allows the system to learn the user's typing throughout the life cycle of the program and the multiple stages of the training functionality which allow the system to get a comprehensive and deep knowledge of how the user types. By changing my original plans and moving away from a database approach due to the high variability in the amount of data produced for each word by the system, I've reduced the performance cost because inserting and removing data from a database is a costly operation in terms of performance.

The word forming and choosing is also another way in which the functionality of the proposed system has been expanded in the produced system. In my proposed system, I planned on checking the entire intervals keystrokes and not forming words. By doing first forming words and then checking a small sample of these, the resulting system is faster and more accurate. Instead of forming one profile for an entire user, which is broad and not particularly accurate as a result, the system produces a mini profile for each word. The more data points per user which are generated by this approach allows the system to be more accurate.

The overall system is accurate and secure, as shown by figure \ref{fig:2DGraph}. The system is the only data that is stored is a highly compressed version of the KDS signal. No data is transmitted off the user's system as well, ensuring that user's who are concerned about the data being shared with a third party such as advertisers can use the system without worry. The addition of the play/pause functionality is also good for users who may be worried about the system recording sensitive information. Therefore, it is clear to see that the produced system is highly accurate and secure. When combined with the weightlessness of the program, its clear to see that the system is fit for use.

However, the produced system does suffer from some shortcomings. As mentioned previously, whilst an imposter user accessing the data stored inside the word files is not a concern due to the nature of the data within, the program suffers from a security standpoint due to the word files being named after the word within. However, because of the play/pause functionality, the user should pause the program every time they need to enter sensitive information which is a small relief from this issue. In the future, I would seek to encrypt the file names in order to mitigate this issue. The validation measure is only as good as the number of data points collected. Therefore, for words such as "hi" which are going to generate relatively few data points, the system struggles with providing an accurate validation score leading to a few cases of false negatives or positives. This is a hard issue to fix however, the obvious solution is simply to omit word with a few data points to be selected for validation. This throes up more problems however, what if small words are the only options in that interval? Another option would be to simply go into greater detail and generate more data points for those words. Unfortunately due to the nature of creating a UI in python, the provided UI for the system is rather ugly and not pleasant to look at. If I was to do this project again this is the main thing I would do differently.

Overall, I would say that producing the system went well with no major problems. The actual element I thought would be hardest and take the most effort in the actual validation system itself was easier than expected. In contrast, planning out the data flow from the raw keystroke data to the validation result took more planning and steps than expected. The UI was more of an issue that it needed to be. This is because  I need to play around with different threads as I needed my program to be able to do two things at once. 

\subsection{Aims and Objective}

In order to judge the success of the project, it is necessary compare the current system against the original aims and objectives of the project which are listed below. 

\begin{enumerate}
	\item To produce a lightweight key logger that can accurately log all inputs accurately and securely
	\item To create a graphical system that allows the user to register or login
	\item At an interval set by the user, the system will create a profile for that user based on their typing since the last interval and check this against the profile created in the registration system
	\item If the user doesn't match, asks the user to re-authenticate
\end{enumerate}

I feel that my project has quite comfortably succeeded in point 1. The key logger produced records all keystrokes along with the action associated with them. Furthermore, the impact on performance is minimal with the keylogger in particular not being noticeable to the user thus satsifying the accurate and lightweight requirements. The validation procedure is unfortunately heavier and may slow down the user system a small amount. If I was to do the project again, I would reduce the performance impact on the user of the validation procedure by either implementing a faster procedure to generate the data points or by improving the storage solution. In order to implement a faster procedure, I would rewrite the project in a faster but more basic language such as C++. This would dramatically improve the speed of the validation procedure but would drastically increase the amount of code written due to the lack of suitable packages. Whilst other storage systems such as databases were explored, I would aim to go for a storage solution that could store highly variable data such as the one produced by this program, securely, speedily and with minimal impact on the user's performance and disk space. One example of a solution is a big data storage system which uses ai in order to store the data. This would also speed up the program because then the compression and decompression functions would no longer be needed which both take up \(O(n)\) time each. Furthermore, this would allow me to implement clustering which would further speed up the program by allowing similar record to be stored next to each other. Overall, the produced system comfortably fulfils this aim.

My project partly succeeded in point 2. I did create a registration system but not in the sense that I meant here. In my original aims and objectives, the registration system would allow the user to sign up using username and password. In my produced system, I quickly dismissed the idea of a login and registration system as explained above. This was because I felt that the need there was no need for a system such as this, because I could just use the windows registration, login and authentication systems. These provide a more secure, fast and overall better experience for the user than any I could create myself. However, I did still create a registration system, this registration system simply was simply used to learn how the user types. This was a massive success and is a relatively complicated system that is explained better in the training section of Design and Implementation. So whilst I didn't create a login system, or the exact registration system I set out in my original aims and objective, I still feel that the produced system fulfils this aim as all the functionality that would have been provided by these systems with a extra provided by the play/pause UI, is in the produced system.

My produced system achieves the aim set out in point 3. It uses intervals and the words typed in those intervals in order to form a profile of sorts for that user which is then compared against the data learnt for that user. However, in the produced system, the user is not able to modify the interval, this is something I would change if I were to make improvements to my program. I would allow the user to choose from a small subset of possible option as allowing the user to choose any time period could easily be abused by an imposter user by setting the period to a huge amount. Bar this, the produced system not only fulfils this aim but eclipses it with the produced system going far beyond this aim.

The final aim is also fulfilled. In my produced system, if the user fails validation, then the whole system is locked using the windows locked screen and the user is asked to re-authenticate thus fulfilling this aim.


\subsection{Time plans}

\section{Social, Sustainability, Commercial and Economic Context}

\section{Personal Development}

Doing this project, enabled me to grow my skill base and as such improve my knowledge of many elements of the project life cycle. Whilst doing this project, I primarily learnt about data and how to manipulate it. I've learnt a lot about a number of different algorithms that I had no clue existed such as the Dynamic Time Warping algorithm in use in my project and the Wigners Distribution that I learnt about it in preparation for this project. I also learnt a lot about the use of git during the development life cycle. One example of how I've applied this new knowledge is my use of issues, branches and merge requests inside my Gitlab. This allowed me to develop new features whilst also safeguarding the work I've already done. Doing the project has also allowed me to further my knowledge of python which has allowed me to code in a more confident manner in the future. I've also developed further independent learning skills such as timekeeping due to the need to keep to deadlines. Doing this project whilst also juggling other university work and a part time job has ensured that I've come out of this project, not only with greater knowledge of   the algorithms and language I was using but also with better time-keeping skills and a more hard working nature.

\begin{enumerate}
	\item Summary and crit analysis
	\begin{itemize}
		\item System works very well - provide examples using test data??
		\item System is lightweight and secure
		\item Compared to og planned, system is more complicated
		\item Struggles with smaller words - less data points
		\item NEED TO COME BACK TO THIS, NOT DETAILED AT ALL
	\end{itemize}
	\item Impact
	\begin{itemize}
		\item Benefits 
		\begin{itemize}
			\item better security in combo with other sec methods
			\item Lightweight and users won't notice
			\item Doesn't spy on people due to only storing KDS and can turn off when user is doing something sensitive
			\item Can be adapted to be used in the real word easily
		\end{itemize}
		\item Risks
		\begin{itemize}
			\item Greater surveillance
			\item Could easily be adapted maliciously - key logger
			\item NEED MORE
		\end{itemize}
	\end{itemize}
	\item Personal Development
	\begin{itemize}
		\item Maths, maths, maths
		\item Further git knowledge???
		\item Exp Project Development
		\item MORE
	\end{itemize}
\end{enumerate}
\chapter{Conclusion}

Comparing the produced system and the planned system brings up a number of differences between the two. Whilst the produced system is more complex as a result of the increased features in play, it still is true to the original aims and objectives. For example, the aims stated the proposed system should be lightweight, accurate and secure and I feel that to a large extent this has been achieved. The aims also stated that the system should have the ability to form profiles for users which I feel as though my system does to an even higher standard than originally stated. The planned system should also have the ability to lock the user out which is inside the completed system. Overall, I believe that the system produced meets the original aims of the project and as such can be judged a success.

\bibliographystyle{ieeetr}
\bibliography{reference}
\end{document}
